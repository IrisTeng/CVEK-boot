#   formula:  A symbolic description of the model to be fitted.
#   label.names: A character string indicating all the interior variables
#                included in each predictor.
#   data: A dataframe to be fitted.
#   method: A character string indicating which Kernel is to be computed.
#   l: A numeric number indicating the hyperparameter of a specific kernel.
#   lambda: A numeric string specifying the range of noise to be chosen.
#           The lower limit of lambda must be above 0.
#
# Returns:
#   test.stat: A numeric number indicating the score test statistics.
# prepare data and estimate under null
y <- data[, as.character(attr(terms(formula), "variables"))[2]]
re <- GenericFormula(formula, label.names)
generic.formula0 <- re[[1]]
len <- re[[2]]
X <- model.matrix(generic.formula0, data)[, -1]
n <- nrow(X)
X1 <- X[, c(1:length(label.names[[1]]))]
X2 <- X[, c((length(label.names[[1]]) + 1):len)]
X12 <- X[, c((len + 1):dim(X)[2])]
# X12 <- NULL
# for (i in 1:length(label.names[[1]])){
#   X12 <- cbind(X12, X1[, i] * X2)
# }
result <- gpr(Y ~ X1 + X2, label.names, data, method, l, lambda, null.hypo = T)
sigma2.n <- result[[1]]
beta0 <- result[[2]]
alpha0 <- result[[3]]
K.gpr <- result[[4]]
noise.hat <- NoiseEstimate(y, sigma2.n, beta0, alpha0, K.gpr)
tau.hat <- noise.hat / sigma2.n
# compute score statistic
Kern <- KernelGenerate(method, l)
K1 <- Kern(X1, X1)
K2 <- Kern(X2, X2)
K0 <- K1 + K2
K12 <- X12 %*% t(X12)
V0 <- tau.hat * K0 + noise.hat * diag(n)
test.stat <- tau.hat * t(y - beta0) %*% ginv(V0) %*% K12 %*% ginv(V0) %*% (y - beta0)
# Jeremiah: heuristic procedure for test statistic correction,
# make sure to delete after adding estimation procedure for sigma
# sigma_true <- 0.1
# tau_true <- sigma_true/sigma2.n
# test.stat <- test.stat/tau_true
return(test.stat)
}
KernelBoot <-
function(formula, label.names, data = NULL,
method = NULL, l = 1, lambda = exp(seq(-5, 5, 1)), B = 100){
# Compute score tests comparing a fitted model and a more general alternative model.
#
# Args:
#   formula:  A symbolic description of the model to be fitted.
#   label.names: A character string indicating all the interior variables
#                included in each predictor.
#   data: A dataframe to be fitted.
#   method: A character string indicating which Kernel is to be computed.
#   l: A numeric number indicating the hyperparameter of a specific kernel.
#   lambda: A numeric string specifying the range of noise to be chosen.
#           The lower limit of lambda must be above 0.
#   B: The number of bootstrap replicates.
#
# Returns:
#   bs.pvalue: A number indicating the test P-value.
# prepare data
Y <- data[, as.character(attr(terms(formula), "variables"))[2]]
# extract the information from the given formula to construct a true formula
generic.formula0 <-
GenericFormula(formula,label.names)$generic.formula
# extract design matrix
X <- model.matrix(generic.formula0, data)[, -1]
n <- nrow(X)
Xm <- colMeans(X)
p <- ncol(X)
X <- X - rep(Xm, rep(n, p))
Xscale <- drop(rep(1 / n, n) %*% X ^ 2) ^ 0.5
X <- X / rep(Xscale, rep(n, p))
X1 <- X[, c(1:length(label.names[[1]]))]
X2 <- X[, c((length(label.names[[1]]) + 1):
(length(label.names[[1]]) + length(label.names[[2]])))]
data0 <- as.data.frame(cbind(Y, X))
result <- gpr(formula, label.names, data0, method, l, lambda, null.hypo = T)
sigma2.n <- result[[1]]
beta0 <- result[[2]]
alpha0 <- result[[3]]
K.gpr <- result[[4]]
noise.hat <- NoiseEstimate(Y, sigma2.n, beta0, alpha0, K.gpr)
mean.Y <- K.gpr %*% alpha0 + beta0
# conduct bootstrap
bs.test <- sapply(1:B, function(k){
Y.star <- mean.Y + rnorm(n, sd = sqrt(noise.hat))
dat <- cbind(Y.star, X)
colnames(dat)[1] <- "Y"
dat <- as.data.frame(dat)
MultiScore2(Y ~ X1 + X2 + X1 * X2, label.names,
data = dat, method, l, lambda)
})
# assemble test statistic
original.test <-
MultiScore2(Y ~ X1 + X2 + X1 * X2, label.names,
data = data0, method, l, lambda)
bs.pvalue <- sum(as.numeric(original.test) <= bs.test) / B
return(list(pvalue = bs.pvalue, bs.test = bs.test, original.test = original.test))
}
LooCV <-
function(y, K.mat, lambda = exp(seq(-5, 5, 1))){
# An implementation of Gaussian processes for regression.
#
# Args:
#   y: A vector of response from original data.
#   K.mat: Kernel matrix calculated from the original data.
#   lambda: A numeric string specifying the range of noise to be chosen.
#           The lower limit of lambda must be above 0.
#
# Returns:
#   lambda0: A numeric number chosen as the best size of noise via LooCV.
# prepare data
n <- nrow(K.mat)
# estimation
CV <- sapply(lambda, function(k){
A <- K.mat %*% ginv(K.mat + k * diag(n))
sum(((diag(n) - A) %*% y / diag(diag(n) - A)) ^ 2)
})
lambda0 <- lambda[which(CV == min(CV))]
A <- K.mat %*% ginv(K.mat + lambda0 * diag(n))
error <- (diag(n) - A) %*% y / diag(diag(n) - A)
return(c(lambda0, error))
}
KernelGenerate <-
function(method = NULL, l = 1, p = 2){
# Generate kernel function.
#
# Args:
#   method: A character string indicating which Kernel is to be computed.
#   l: A numeric number indicating the hyperparameter of a specific kernel.
#
# Returns:
#   Kern: A matrix indicating the expected kernel function.
if (method == "linear")
SE <- function(xp, xq, l) t(xp) %*% xq
else if (method == "polynomial")
SE <- function(xp, xq, l) (t(xp) %*% xq + 1) ^ p
else if (method == "rbf")
SE <- function(xp, xq, l) exp(- sum((xp - xq) ^ 2) / (2 * l ^ 2))
else
stop("method must be linear, polynomial or rbf!")
Kern <- function(X2, X1) apply(X1, 1, function(xp){
apply(X2, 1, function(xq){
SE(xp, xq, l = l)
})
})
return(Kern)
}
gpr <-
function(formula, label.names,
data = NULL, method = NULL, l = 1,
lambda = exp(seq(-5, 5, 1))){
# An implementation of Gaussian processes for regression.
#
# Args:
#   formula:  A symbolic description of the model to be fitted.
#   label.names: A character string indicating all the interior variables
#                included in each predictor.
#   data: A dataframe to be fitted.
#   method: A character string indicating which Kernel is to be computed.
#   l: A numeric number indicating the hyperparameter of a specific kernel.
#   lambda: A numeric string specifying the range of noise to be chosen.
#           The lower limit of lambda must be above 0.
#   null.hypo: logical. To indicate whether the Kernel function is calculated
#              under the null hypothesis.
#
# Returns:
#   sigma.n: A numeric number chosen as the best size of noise via LooCV.
#   intercept: A numberic number indicating the bias weight.
#   alpha: A numeric string indicating the projection vector.
# prepare data
y <- data[, as.character(attr(terms(formula), "variables"))[2]]
# extract the information from the given formula to construct a true formula
re <- GenericFormula(formula, label.names)
generic.formula0 <- re[[1]]
len <- re[[2]]
# extract design matrix
X <- model.matrix(generic.formula0, data)[, -1]
n <- nrow(X)
# estimation
Kern <- KernelGenerate(method, l)
X1 <- X[, c(1:length(label.names[[1]]))]
X2 <- X[, c((length(label.names[[1]]) + 1):len)]
K <- Kern(X1, X1) + Kern(X2, X2)
if (length(lambda) == 1)
lambda0 = 0
else
lambda0 <- LooCV(y, K, lambda)[[1]]
K1 <- cbind(1, K)
K2 <- cbind(0, rbind(0, K))
theta <- ginv(lambda0 * K2 + t(K1) %*% K1) %*% t(K1) %*% y
beta0 <- theta[1]
alpha <- theta[-1]
return(list(sigma2.n = lambda0, intercept = beta0, alpha = alpha, K = K))
}
library(mvtnorm)
library(MASS)
library(snowfall)
library(psych)
library(limSolve)
formula <- Y~X1+X2
label.names <- list(X1=c("x1","x2"),X2=c("x3","x4"))
rawData <- OriginalData2(100, label.names, "linear", int.effect = 0)
Kernlist <- NULL
# linear kernel
Kernlist <- c(Kernlist, KernelGenerate('linear'))
# polynomial kernel, p = 2
Kernlist <- c(Kernlist, KernelGenerate('polynomial', p = 2))
# polynomial kernel, p = 3
Kernlist <- c(Kernlist, KernelGenerate('polynomial', p = 3))
# polynomial kernel, p = 4
Kernlist <- c(Kernlist, KernelGenerate('polynomial', p = 4))
# rbf kernel, l = .3
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = .3))
# rbf kernel, l = .6
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = .6))
# rbf kernel, l = 1
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = 1))
# rbf kernel, l = 2
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = 2))
# rbf kernel, l = 3
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = 3))
# rbf kernel, l = 4
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = 4))
# rbf kernel, l = 4
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = 5))
# rbf kernel, l = 4
Kernlist <- c(Kernlist, KernelGenerate('rbf', l = 6))
Ensemble <-
function(formula, label.names, Kernlist,
data = NULL, lambda = exp(seq(-5, 5, 1))){
y <- data[, as.character(attr(terms(formula), "variables"))[2]]
# extract the information from the given formula to construct a true formula
re <- GenericFormula(formula, label.names)
generic.formula0 <- re[[1]]
len <- re[[2]]
# extract design matrix
X <- model.matrix(generic.formula0, data)[, -1]
n <- nrow(X)
# estimation
# D = 10
X1 <- X[, c(1:length(label.names[[1]]))]
X2 <- X[, c((length(label.names[[1]]) + 1):len)]
D <- length(Kernlist)
lambda.mat <- matrix(0, nrow = D, ncol = n + 1)
for (i in 1:D){
Kern <- Kernlist[[i]]
K <- Kern(X1, X1) + Kern(X2, X2)
if (length(lambda) != 1)
lambda.mat[i, ] <- LooCV(y, K, lambda)
}
A <- t(lambda.mat[, 2:(n + 1)])
B <- rep(0, 100)
E <- rep(1, D)
F <- 1
G <- diag(D)
H <- rep(0, D)
u.hat <- lsei(A, B, E = E, F = F, G = G, H = H)$X
return(u.hat)
}
data <- OriginalData2(100, label.names, 'rbf')
h <- Ensemble(formula, label.names, data)
h <- Ensemble(formula, label.names, Kernlist, data)
h
y <- data[, as.character(attr(terms(formula), "variables"))[2]]
# extract the information from the given formula to construct a true formula
re <- GenericFormula(formula, label.names)
generic.formula0 <- re[[1]]
len <- re[[2]]
# extract design matrix
X <- model.matrix(generic.formula0, data)[, -1]
n <- nrow(X)
# estimation
# D = 10
X1 <- X[, c(1:length(label.names[[1]]))]
X2 <- X[, c((length(label.names[[1]]) + 1):len)]
D <- length(Kernlist)
lambda.mat <- matrix(0, nrow = D, ncol = n + 1)
for (i in 1:D){
Kern <- Kernlist[[i]]
K <- Kern(X1, X1) + Kern(X2, X2)
if (length(lambda) != 1)
lambda.mat[i, ] <- LooCV(y, K, lambda)
}
lambda = exp(seq(-5, 5, 1))
for (i in 1:D){
Kern <- Kernlist[[i]]
K <- Kern(X1, X1) + Kern(X2, X2)
if (length(lambda) != 1)
lambda.mat[i, ] <- LooCV(y, K, lambda)
}
View(lambda.mat)
h]
h
data <- OriginalData2(100, label.names, 'linear')
h <- Ensemble(formula, label.names, Kernlist, data)
y <- data[, as.character(attr(terms(formula), "variables"))[2]]
# extract the information from the given formula to construct a true formula
re <- GenericFormula(formula, label.names)
generic.formula0 <- re[[1]]
len <- re[[2]]
# extract design matrix
X <- model.matrix(generic.formula0, data)[, -1]
n <- nrow(X)
# estimation
# D = 10
X1 <- X[, c(1:length(label.names[[1]]))]
X2 <- X[, c((length(label.names[[1]]) + 1):len)]
D <- length(Kernlist)
lambda.mat <- matrix(0, nrow = D, ncol = n + 1)
for (i in 1:D){
Kern <- Kernlist[[i]]
K <- Kern(X1, X1) + Kern(X2, X2)
if (length(lambda) != 1)
lambda.mat[i, ] <- LooCV(y, K, lambda)
}
View(lambda.mat)
h
OriginalData2 <-
function(size, label.names, method = NULL, int.effect = 0, eps = 0.01){
# Generate original data.
#
# Args:
#   size: A numeric number specifying the number of observations.
#   label.names: A character string indicating all the interior variables
#                included in each predictor.
#   method: A character string indicating which Kernel is to be computed.
#   int.effect: A numeric number specifying the size of interaction.
#   eps: A numeric number indicating the size of noise.
#
# Returns:
#   data: A dataframe including response and predictors.
# X~N(0,I)
X1 <- rmvnorm(n = size,
mean = rep(0, length(label.names[[1]])),
sigma = diag(length(label.names[[1]])))
X2 <- rmvnorm(n = size,
mean = rep(0, length(label.names[[2]])),
sigma = diag(length(label.names[[2]])))
# X.int <- NULL
# for (i in 1:length(label.names[[1]])){
#   X.int <- cbind(X.int, X1[, i] * X2)
# }
Kern <- KernelGenerate(method)
w1 <- rnorm(size)
# w2 <- rnorm(size)
w2 <- w1
w12 <- rnorm(size)
K1 <- Kern(X1, X1)
K2 <- Kern(X2, X2)
h0 <- K1 %*% w1 + K2 %*% w2
# h0 <- h0 / sqrt(sum(h0 ^ 2))
#
# h1.prime <- (K1 * K2) %*% w12
# Ks <- svd(Kern(X1, X1) + Kern(X2, X2))
# if(length(Ks$d / sum(Ks$d) > 0.001) > 0){
#   len <- length(Ks$d[Ks$d / sum(Ks$d) > 0.001])
#   U0 <- Ks$u[, 1:len]
#   h1.prime.hat <- fitted(lm(h1.prime ~ U0))
#   h1 <- h1.prime - h1.prime.hat
#   if(all(h1 == 0)){
#     stop("interaction term colinear with main-effect space!")
#     h1 <- h1.prime
#     h1 <- h1 / sqrt(sum(h1 ^ 2))
#   }
#   else
#     h1 <- h1 / sqrt(sum(h1 ^ 2))
# }
# else{
#   stop("largest eigen value smaller than 0.001!")
#   h1 <- h1.prime
#   h1 <- h1 / sqrt(sum(h1 ^ 2))
# }
h1 <- (K1 * K2) %*% w12
Y <- h0 + int.effect * h1 + rnorm(size, 0, eps)
data <- as.data.frame(cbind(Y, X1, X2))
colnames(data) <- c("Y", label.names[[1]], label.names[[2]])
return(data)
}
data <- OriginalData2(100, label.names, 'linear')
h <- Ensemble(formula, label.names, Kernlist, data)
h
data <- OriginalData2(100, label.names, 'polynomial')
h <- Ensemble(formula, label.names, Kernlist, data)
h
data <- OriginalData2(100, label.names, 'rbf')
h <- Ensemble(formula, label.names, Kernlist, data)
h
OriginalData2 <-
function(size, label.names, method = NULL, int.effect = 0, eps = 0.01){
# Generate original data.
#
# Args:
#   size: A numeric number specifying the number of observations.
#   label.names: A character string indicating all the interior variables
#                included in each predictor.
#   method: A character string indicating which Kernel is to be computed.
#   int.effect: A numeric number specifying the size of interaction.
#   eps: A numeric number indicating the size of noise.
#
# Returns:
#   data: A dataframe including response and predictors.
# X~N(0,I)
X1 <- rmvnorm(n = size,
mean = rep(0, length(label.names[[1]])),
sigma = diag(length(label.names[[1]])))
X2 <- rmvnorm(n = size,
mean = rep(0, length(label.names[[2]])),
sigma = diag(length(label.names[[2]])))
# X.int <- NULL
# for (i in 1:length(label.names[[1]])){
#   X.int <- cbind(X.int, X1[, i] * X2)
# }
Kern <- KernelGenerate(method)
w1 <- rnorm(size)
# w2 <- rnorm(size)
w2 <- w1
w12 <- rnorm(size)
K1 <- Kern(X1, X1)
K2 <- Kern(X2, X2)
h0 <- K1 %*% w1 + K2 %*% w2
h0 <- h0 / sqrt(sum(h0 ^ 2))
h1.prime <- (K1 * K2) %*% w12
Ks <- svd(Kern(X1, X1) + Kern(X2, X2))
if(length(Ks$d / sum(Ks$d) > 0.001) > 0){
len <- length(Ks$d[Ks$d / sum(Ks$d) > 0.001])
U0 <- Ks$u[, 1:len]
h1.prime.hat <- fitted(lm(h1.prime ~ U0))
h1 <- h1.prime - h1.prime.hat
if(all(h1 == 0)){
stop("interaction term colinear with main-effect space!")
h1 <- h1.prime
h1 <- h1 / sqrt(sum(h1 ^ 2))
}
else
h1 <- h1 / sqrt(sum(h1 ^ 2))
}
else{
stop("largest eigen value smaller than 0.001!")
h1 <- h1.prime
h1 <- h1 / sqrt(sum(h1 ^ 2))
}
# h1 <- (K1 * K2) %*% w12
Y <- h0 + int.effect * h1 + rnorm(size, 0, eps)
data <- as.data.frame(cbind(Y, X1, X2))
colnames(data) <- c("Y", label.names[[1]], label.names[[2]])
return(data)
}
h <- Ensemble(formula, label.names, Kernlist, data)
h
data <- OriginalData2(100, label.names, 'linear')
h <- Ensemble(formula, label.names, Kernlist, data)
h
data <- OriginalData2(100, label.names, 'polynomial')
h <- Ensemble(formula, label.names, Kernlist, data)
h
data <- OriginalData2(100, label.names, 'polynomial')
h <- Ensemble(formula, label.names, Kernlist, data)
h
data <- OriginalData2(200, label.names, 'polynomial')
h <- Ensemble(formula, label.names, Kernlist, data)
data <- OriginalData2(100, label.names, 'polynomial')
h <- Ensemble(formula, label.names, Kernlist, data)
h
data <- OriginalData2(150, label.names, 'polynomial')
h <- Ensemble(formula, label.names, Kernlist, data)
Ensemble <-
function(formula, label.names, Kernlist,
data = NULL, lambda = exp(seq(-5, 5, 1))){
y <- data[, as.character(attr(terms(formula), "variables"))[2]]
# extract the information from the given formula to construct a true formula
re <- GenericFormula(formula, label.names)
generic.formula0 <- re[[1]]
len <- re[[2]]
# extract design matrix
X <- model.matrix(generic.formula0, data)[, -1]
n <- nrow(X)
# estimation
# D = 10
X1 <- X[, c(1:length(label.names[[1]]))]
X2 <- X[, c((length(label.names[[1]]) + 1):len)]
D <- length(Kernlist)
lambda.mat <- matrix(0, nrow = D, ncol = n + 1)
for (i in 1:D){
Kern <- Kernlist[[i]]
K <- Kern(X1, X1) + Kern(X2, X2)
if (length(lambda) != 1)
lambda.mat[i, ] <- LooCV(y, K, lambda)
}
A <- t(lambda.mat[, 2:(n + 1)])
B <- rep(0, n)
E <- rep(1, D)
F <- 1
G <- diag(D)
H <- rep(0, D)
u.hat <- lsei(A, B, E = E, F = F, G = G, H = H)$X
return(u.hat)
}
data <- OriginalData2(200, label.names, 'polynomial')
h <- Ensemble(formula, label.names, Kernlist, data)
h
sum(h)
sum(h^2)
?get
get("%o%")
?with
