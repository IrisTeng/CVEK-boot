\documentclass[11pt]{article}

% change document font family to Palatino, and code font to Courier
\usepackage{mathpazo} % add possibly `sc` and `osf` options
\usepackage{eulervm}
\usepackage{courier}
%allow formula formatting
\usepackage{amsmath}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{comment}
\usepackage{enumerate}
\usepackage{xcolor}
%title positon
\usepackage{titling} %fix title
\setlength{\droptitle}{-6em}   % Move up the title 
%change section title font size
\usepackage{titlesec} 
\titleformat{\section}
  {\normalfont\fontsize{12}{15}}{\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\fontsize{12}{13}}{\thesection}{1em}{}
\usepackage{verbatim}
% change page margin
\usepackage[margin=1 in]{geometry} 
\usepackage{subfigure}
%allow inserting multiple graphs
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
%allow code chunks
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{frame=lrbt,xleftmargin=\fboxsep, xrightmargin=-\fboxsep}

%declare sum int sign
\DeclareMathOperator*{\SumInt}{%
\mathchoice%
  {\ooalign{$\displaystyle\sum$\cr\hidewidth$\displaystyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.14\height}{\scalebox{.7}{$\textstyle\sum$}}\cr\hidewidth$\textstyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
}

\usepackage{"./macro/GrandMacros"}
\usepackage{"./macro/Macro_BIO235"}
\begin{document}
\setlength{\parindent}{0pt}
%\setcounter{equation}{0}
%\renewcommand{\theequation}{1.1.\arabic{equation}}
%\paragraph{AIC and small sample correction}\mbox{}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% TItle page with contents %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{CVEK-boot Report\\Wenying Deng \vspace{-1ex}}

\pretitle{\begin{flushright}\normalsize}
\posttitle{\par\end{flushright}}
\author{}
\date{}
\vspace{-10em}
\maketitle
\vspace{-8em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Formal Sections %%%%% %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6em}
\section*{{\bf 1. Introduction}}
\section*{{\bf 2. Method}}
\subsection*{{1. CVEK}}
\setcounter{equation}{0}
\renewcommand{\theequation}{1.1.\arabic{equation}}
\subsubsection*{{1.1. main algorithm}}
We use the classical variance component test to construct a testing procedure for the hypothesis about Gaussian process function:
\begin{align}
H_0: h \in \Hsc_0.
\end{align}
We first translate above hypothesis into a hypothesis in terms of model parameters. The key of our approach is to assume that $h$ lies in a RKHS generated by a \textsl{garrote kernel function}
$k_{\delta}(\bz, \bz')$, which is constructed by including an extra \textsl{garrote parameter} $\delta$ to a given kernel function. When $\delta=0$, the garrote kernel function $k_0(\bx, \bx')=k_{\delta}(\bx, \bx')\mid _{\delta=0}$ generates exactly $\Hsc_0$, the space of functions under the null hypothesis. In order to adapt this general hypothesis to their hypothesis of interest, practitioners need only to specify the form of the garrote kernel so that $\Hsc_0$ corresponds to the null hypothesis. For example, if $k_{\delta}(\bx)=k(\delta * x_1, x_2,..., x_p), \delta=0$ corresponds to the null hypothesis $H_0: h(\bx)=h(x_2,...,x_p)$, i.e. the function $h(\bx)$ does not depend on $x_1$. As a result, the general hypothesis is equivalent to:
\begin{align}
H_0: \delta=0.
\end{align}
We now construct a test statistic $\hat{T}_0$ for (1.1.2) by noticing that the garrote parameter $\delta$ can be treated as a variance component parameter in the linear mixed model. This is because the Gaussian process under garrote kernel can be formulated into below LMM:
\begin{align*}
\by=\bmu+\bh+\bepsilon \quad where \quad \bh \sim N(\mathbf{0}, \tau \bK_\delta) \quad \bepsilon \sim N(\mathbf{0}, \sigma^2\bI)
\end{align*}
where $\bK_\delta$ is the kernel matrix generated by $k_{\delta}(\bz, \bz')$. Consequently, we can derive a variance component test for $H_0$ by calculating the square derivative of $L_{REML}$ with respect to $\delta$ under $H_0$:
\begin{align}
\hat{T}_0=\hat{\tau}*(\by-\hat{\bmu})^T\bV_0^{-1}[\partial \bK_0]\bV_0^{-1}(\by-\hat{\bmu})
\end{align}
where $\bV_0=\hat{\sigma}^2\bI+\hat{\tau}\bK_0$. In this expression, $\bK_0=\bK_\delta \mid_{\delta=0}$, and $\partial \bK_0$ is the null derivative kernel matrix whose $(i, j)^{th}$ entry is $\frac{\partial }{\partial \delta}k_\delta(\bx, \bx') \mid_{\delta=0}$.
As discussed previously, misspecifying the null kernel function $k_0$ negatively impacts the performance of the resulting hypothesis test. To better understand the mechanism at play, we express the test statistic $\hat{T}_0$ from (1.1.3) in terms of the model residual $\hat{\bepsilon}=\by-\hat{\bmu}-\hat{\bh}$:
\begin{align}
\hat{T}_0=(\frac{\hat{\tau}}{\hat{\sigma}^4})*\hat{\bepsilon}^T[\partial \bK_0]\hat{\bepsilon},
\end{align}
where we have used the fact $\bV_0^{-1}(\by-\hat{\bmu})=(\hat{\sigma}^2)^{-1}(\hat{\bepsilon})$. As shown, the test statistic $\hat{T}_0$ is a scaled quadratic-form statistic that is a function of the model residual. If $k_0$ is too restrictive, model estimates will \textbf{underfit} the data even under the null hypothesis, introducing extraneous correlation among the $\hat{\epsilon}_i$'s, therefore leading to overestimated $\hat{T}_0$ and eventually underestimated p-valueunder under the null. In this case, the test procedure will frequently reject the null hypothesis (i.e. suggest the existence of nonlinear interaction) even when there is in fact no interaction, yielding an invalid test due to \textbf{inflated Type I error}. On the other hand, if $k_0$ is too flexible, model estimates will likely \textbf{overfit} the data in small samples, producing underestimated residuals, an underestimated test statistic, and overestimated p-values. In this case, the test procedure will too frequently fail to reject the null hypothesis (i.e. suggesting there is no interaction) when there is in fact interaction, yielding an insensitive test with \textbf{diminished power}.\\
The null distribution of $\hat{T}$ can be approximated using a scaled chi-square distribution $\kappa \chi_\nu^2$ using Satterthwaite method by matching the first two moments of $T$:
\begin{align*}
\kappa * \nu=E(T), \quad 2*\kappa^2*\nu=Var(T)
\end{align*}
with solution:
\begin{align*}
\hat{\kappa}=\hat{\bI}_{\delta\delta}/[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)] \quad \hat{\nu}=[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)]^2/(2*\hat{\bI}_{\delta\delta})
\end{align*}
where $\hat{\bI}_{\delta\theta}$ and $\hat{\bI}_{\delta\theta}$ are the submatrices of the REML information matrix. Numerically more accurate, but computationally less efficient approximation methods are also available.\\
Finally, the p-value of this test is calculated by examining the tail probability of $\hat{\kappa} \chi_{\hat{\nu}}^2$:
\begin{align*}
p=P(\hat{\kappa} \chi_{\hat{\nu}}^2>\hat{T})=P(\chi_{\hat{\nu}}^2>\hat{T}/\hat{\kappa})
\end{align*}
A complete summary of the proposed testing procedure is available in Algorithm 1.\\
In light of the discussion about model misspecification in Introduction section, we highlight the fact that our proposed test (1.1.3) is robust against model misspecification under the alternative, since the calculation of test statistics do not require detailed parametric assumption about $k_\delta$. However, the test is NOT robust against model misspecification under the null, since the expression of both test statistic $\hat{T}_0$ and the null distribution parameters $(\hat{\kappa}, \hat{\nu})$ still involve the kernel matrices generated by $k_0$ (see Algorithm 1). We address this problem by proposing a robust estimation procedure for the kernel matrices under the null.
\begin{table}[]
\centering
\begin{tabular}{lll}
\hline
&\textbf{Algorithm 1} Variance Component Test for $h \in \Hsc_0$ \\ \hline
1: &\textbf{procedure} VCT FOR INTERACTION\\
&\textbf{Input:} Null Kernel Matrix $\bK_0$, Derivative Kernel Matrix $\partial \bK_0$, Data $(y, \bX)$\\
&\textbf{output:} Hypothesis Test p-value $p$\\
&\#Step 1: \quad Estimate Null Model using REML\\
2: \quad &$(\hat{\bmu}, \hat{\tau}, \hat{\sigma}^2)=argmin L_{REML}(\bmu, \tau, \sigma^2|\bK_0)$\\
&\#Step 2: \quad Compute Test Statistic and Null Distribution Parameters\\
3: \quad &$\hat{T}_0=\hat{\tau}*(\by-\bX\hat{\bbeta})^T\bV_0^{-1}\partial \bK_0\bV_0^{-1}(\by-\bX\hat{\bbeta})$\\
4: \quad &$\hat{\kappa}=\hat{\bI}_{\delta\delta}/[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)]$, \quad $\hat{\nu}=[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)]^2/(2*\hat{\bI}_{\delta\delta})$\\
&\#Step 3: \quad Compute p-value and reach conclusion\\
5: \quad &$p=P(\hat{\kappa} \chi_{\hat{\nu}}^2>\hat{T})=P(\chi_{\hat{\nu}}^2>\hat{T}/\hat{\kappa})$\\
6: &\textbf{end procedure}\\
\hline
\end{tabular}
\end{table}


\setcounter{equation}{0}
\renewcommand{\theequation}{1.2.\arabic{equation}}
\subsubsection*{{1.2. tuning parameter selection}}
Models may provide a good fit to the training data, but it will not fit sufficiently well to the test data. Tuning parameter could be chosen to address this problem. Here we define four objective functions in terms of tuning parameter $\lambda \in \Lambda$ to be minimized. Denote
\begin{align}
\bP_\lambda=\bK(\bX, \bX)[\bK(\bX, \bX)+\lambda \bI]^{-1}
\end{align}
In this way, $Trace(\bP_\lambda)$ is the effective number of model parameters, excluding $\beta_0$ and $\sigma^2$. It decreases monotonically with $\lambda>0$. 

\paragraph{cross-validation: K fold and loo$^{1}$}\mbox{}\\
Cross validation is probably the simplest and most widely used method for estimating prediction error. Suppose we do a $K$-fold cross-validation, which partitions observations into $K$ groups, $\kappa(1),...,\kappa(K)$, and calculates $\bbeta_\lambda$ $K$ times, each time leaving out group $\kappa(i)$, to get $\bbeta_\lambda^{-\kappa(1)}, \bbeta_\lambda^{-\kappa(2)}$, etc. For $\bbeta_\lambda^{-\kappa(i)}$, cross-validated residuals are calculated on the observations in $\kappa(i)$, which did not contribute to estimating $\bbeta$. The objective function estimated prediction error and is the sum of the squared cross-validated residuals:
\begin{align}
\lambda_{K-CV}=\underset{\lambda \in \Lambda}{argmin}\;ln\sum_{i=1}^K(\by_{\kappa(i)}-\Phi(\bx_{\kappa(i)})\bbeta_\lambda^{-\kappa(i)})^T(\by_{\kappa(i)}-\Phi(\bx_{\kappa(i)})\bbeta_\lambda^{-\kappa(i)})
\end{align}
LooCV is the situation when $K=n$. In this case, we can write our objective function as:
\begin{align}
\lambda_{n-CV}=\underset{\lambda \in \Delta}{argmin}\;ln\sum_{i=1}^n \frac{(Y_i-\phi(X_i)^T\bbeta_\lambda)^2}{(1-P_{\lambda[ii]}-\frac{1}{n})^2}
\end{align}
The value $K$ influences bias and variance of cross-validation. With $K=n$, the cross-validation estimator is approximately unbiased for the true (expected) prediction error because it almost use all the data in each training set. Therefore, it can have high variance because $n$ training sets are so similar to one another. Additionally, the computational burden is also considerable, requiring $n$ applications of the learning method. On the other hand, with larger $K$ such as 5 or 10, cross-validation will have lower variance, but making bias a problem.


\paragraph{AIC and small sample correction}\mbox{}\\
Based on the idea of "model fit + model complexity", Akaike's Information Criterion (AIC) choose $\lambda$ by minimizing,
\begin{align*}
AIC=&2(p+2)-2ln(\hat{L})\\
=&2(p+2)-2[-\frac{n}{2}ln (\hat{\sigma}^2)-\frac{n}{2}ln (2\pi)-\frac{1}{2\hat{\sigma}^2}\by^T(\bI_n-\bP_\lambda)^2\by]\\
=&2(p+2)+n ln[\frac{1}{n}\by^T(\bI_n-\bP_\lambda)^2\by]+n+n ln(2\pi)
\end{align*}
Drop the constant $n$ and divide it by $n$, we obtain our objective function:
\begin{align}
\lambda_{AIC}=\underset{\lambda \in \Lambda}{argmin}\{ln\; \by^T(\bI_n-\bP_\lambda)^2\by+\frac{2(Trace(\bP_\lambda)+2))}{n}\}
\end{align}
When $n$ is small, extreme overfitting is possible, giving small bias/ large variance estimates. The small-sample correction of AIC is derived by minimizing minus 2 times expected log likelihood, where we plug in $\bP_\lambda$ and $\hat{\sigma}^2$. In this case, we obtain our small-sample size objective function AICc:
\begin{align}
\lambda_{AICc}=\underset{\lambda \in \Lambda}{argmin}\{ln\; \by^T(\bI_n-\bP_\lambda)^2\by+\frac{2(Trace(\bP_\lambda)+2))}{n-Trace(\bP_\lambda)-3}\}
\end{align}
Compare (1.2.4) and (1.2.5), it is easy to tell that AICc considers more of the model complexity since $\frac{n}{n-Trace(\bP_\lambda)-3}>1$. It makes sense intuitively because it need to shrink more to prevent small bias/ large variance estimates.

\paragraph{GCV and small sample correction}\mbox{}\\
In (1.2.3), if we approximate each $P_{\lambda[ii]}$ with their mean $\frac{Trace(\bP_\lambda)}{n}$, in a sense that we give equal weight to all observations. We get our GCV objective function:
\begin{align}
\lambda_{GCV}=\underset{\lambda \in \Lambda}{argmin}\{ln\; \by^T(\bI_n-\bP_\lambda)^2\by-2ln(1-\frac{Trace(\bP_\lambda)}{n}-\frac{1}{n})\}
\end{align}
The "$-\frac{1}{n}$" terms in (1.2.6) is because GCV counts $\beta_0$ as part of model complexity, but not $\sigma^2$. This motivates the proposed small-sample correction to GCV, which does count $\sigma^2$ as a parameter:
\begin{align}
\lambda_{GCVc}=\underset{\lambda \in \Lambda}{argmin}\{ln\; \by^T(\bI_n-\bP_\lambda)^2\by-2ln(1-\frac{Trace(\bP_\lambda)}{n}-\frac{2}{n})_+\}
\end{align}
Under this situation, perfect fit of the observations to the predictions, given by $\lambda=0$, cannot occur.

\paragraph{GMPML-based selection}\mbox{}\\
If we assume $\bbeta$ are jointly and independently normal with mean zero and variance $\sigma^2/\lambda$, the penalty term matches the negative normal log-density, up to a normalizing constant not depending on $\bbeta$:
\begin{align*}
p_\lambda(\bbeta, \sigma^2)=\frac{\lambda}{2\sigma^2}\bbeta^T\bbeta-\frac{p}{2}ln(\lambda)+\frac{p}{2}ln(\sigma^2)
\end{align*}
One can consider a marginal likelihood, where $\lambda$ is interpreted as the variance component of a mixed-effects model:
\begin{align*}
m(\lambda, \sigma^2)=&ln \int_{\bbeta} exp\{ l(\bbeta, \sigma^2)-p_\lambda(\bbeta, \sigma^2)\}d\bbeta \\=&-\frac{1}{2\sigma^2}\by^T(\bI_n-\bP_\lambda)\by-\frac{n}{2}ln(\sigma^2)+\frac{1}{2}ln \mid \bI_n-\bP_\lambda \mid
\end{align*}
From this, $\by \mid \lambda, \sigma^2$ is multivariate normal with mean $\mathbf{0}_n$ and covariance $\sigma^2(\bI_n-\bP_\lambda)^{-1}$. The maximum profile marginal likelihood (MPML) estimate profiles $m(\lambda, \sigma^2)$ over $\sigma^2$, replacing each instance with $\hat{\sigma}_\lambda^2=\by^T(\bI_n-\bP_\lambda)\by/n$, and maximized the "concentrated" log-likelihood, $m(\lambda, \hat{\sigma}_\lambda^2)$:
\begin{align*}
\lambda_{MPML}=\underset{\lambda \in \Lambda}{argmin}\{ln\; \by^T(\bI_n-\bP_\lambda)\by-\frac{1}{n}ln \mid \bI_n-\bP_\lambda \mid\}
\end{align*}
Generalized MPML adjusts the penalty to account for estimation of regression parameter $\beta_0$ that is not marginalized, resulting in one degree of freedom:
\begin{align}
\lambda_{GMPML}=\underset{\lambda \in \Lambda}{argmin}\{ln\; \by^T(\bI_n-\bP_\lambda)\by-\frac{1}{n-1}ln \mid \bI_n-\bP_\lambda \mid\}
\end{align}

\vspace{2em}
\paragraph{Discussion}\mbox{}\\
????
\begin{align*}
\mid \bI_n-\bP_\lambda \mid^{\frac{1}{n}}, \quad (1-\frac{Trace(\bP_\lambda)}{n}-\frac{1}{n})^2
\end{align*}

\setcounter{equation}{0}
\renewcommand{\theequation}{1.3.\arabic{equation}}
\subsubsection*{{1.3. ensemble strategy}}
Observation in (1.1.4) motivates the need for a kernel estimation strategy that is \textsl{flexible} so that it does not underfit under the null, yet \textsl{stable} so that it does not overfit under the alternative. To this end, we propose estimating $h$ using the ensemble of a library of fixed base kernels $\{k_d\}_{d=1}^D$:
\begin{align}
\hat{h}(\bx)=\sum_{d=1}^D u_d\hat{h}_d(\bx), \quad \bu \in \Delta=\{\bu | \bu \geq 0, \parallel \bu \parallel_1=1\}
\end{align}
where $\hat{h}_d$ is the kernel predictor generated by $d^{th}$ base kernel $k_d$.\\
To be more specific, for each given basis kernel $\{k_d\}_{d=1}^D$, we first estimate $\hat{\bh}_d=\bK_d(\bK_d+\hat{\lambda}_d\bI)^{-1}\by$, the prediction based on $d^{th}$ kernel, where the tuning parameter $\hat{\lambda}_d$ is selected by minimizing one of the four objective functions given in section 1.2. Denote the estimated error for $d^{th}$ kernel as $\hat{\epsilon}_d$ and $\bA_{d,\lambda}=\bK_d(\bK_d+\lambda\bI)^{-1}$.
\paragraph{cross-validation}\mbox{}\\
???\\
After obtaining the estimated errors $\{\hat{\epsilon}_d\}_{d=1}^D$, we estimate the ensemble weights $\bu=\{u_d\}_{d=1}^D$ such that it minimizes the overall error:
\begin{align*}
\hat{\bu}=\underset{\bu \in \Delta}{argmin}\parallel \sum_{d=1}^Du_d\hat{\epsilon}_d\parallel^2 \quad where \Delta=\{\bu | \bu \geq 0, \parallel \bu \parallel_1=1\}
\end{align*}
Then produce the final ensemble prediction:
\begin{align*}
\hat{\bh}=\sum_{d=1}^D \hat{u}_d \bh_d=\sum_{d=1}^D \hat{u}_d \bA_{d,\hat{\lambda}_d}\by=\hat{\bA}\by
\end{align*}
where $\hat{\bA}=\sum_{d=1}^D \hat{u}_d \bA_{d,\hat{\lambda}_d}$ is the ensemble matrix.

\paragraph{simple averaging}\mbox{}\\
Another way to obtain the ensemble matrix would be simply choose $u_d=1\D$ for $d=1,2,...D$.\\
????

\vspace{2em}
Now that we have the ensemble matrix $\hat{\bA}$, we can estimate the ensemble kernel matrix $\hat{\bK}$ by solving:
\begin{align*}
\hat{\bK}(\hat{\bK}+\lambda \bI)^{-1}=\hat{\bA}
\end{align*}
Specifically, if we denote $\bU_A$ and $\{\delta_{A,k}\}_{k=1}^n$ the eigenvector and eigenvalues of $\hat{\bA}$, then $\hat{\bK}$ adopts the form:
\begin{align*}
\hat{\bK}=\bU_A diag(\frac{\delta_{A,k}}{1-\delta_{A,k}})\bU_A^T
\end{align*}



\subsubsection*{{1.4. kernel choice}}
\paragraph{types of kernel}\mbox{}\\
\paragraph{characterization as a function class}\mbox{}\\
\paragraph{spectral property}\mbox{}\\
\subsection*{{2. Hypothesis Test}}
\subsubsection*{{2.1. Asymptotic Test}}
\subsubsection*{{2.2. Bootstrap Test}}
\section*{{\bf 3. Simulation}}
\section*{{\bf 4. Conclusion}}















%----------------------------------------------------------------------------------------
\end{document}