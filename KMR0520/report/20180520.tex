\documentclass[11pt]{article}

% 
\usepackage{natbib}
\bibliographystyle{unsrtnat}

% change document font family to Palatino, and code font to Courier
\usepackage{mathpazo} % add possibly `sc` and `osf` options
\usepackage{eulervm}
\usepackage{courier}
%allow formula formatting
\usepackage{amsmath}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{comment}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
%title positon
\usepackage{titling} %fix title
\setlength{\droptitle}{-6em}   % Move up the title 

\usepackage{verbatim}
% change page margin
\usepackage[margin=1 in]{geometry} 
\usepackage{subfigure}
%allow inserting multiple graphs
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
%allow code chunks
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{frame=lrbt,xleftmargin=\fboxsep, xrightmargin=-\fboxsep}

%declare sum int sign
\DeclareMathOperator*{\SumInt}{%
\mathchoice%
  {\ooalign{$\displaystyle\sum$\cr\hidewidth$\displaystyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.14\height}{\scalebox{.7}{$\textstyle\sum$}}\cr\hidewidth$\textstyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
}

\usepackage{"./macro/GrandMacros"}
\usepackage{"./macro/Macro_BIO235"}
\begin{document}
\setlength{\parindent}{0pt}
%\setcounter{equation}{0}
%\renewcommand{\theequation}{1.1.\arabic{equation}}
%\paragraph{AIC and small sample correction}\mbox{}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% TItle page with contents %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{CVEK-boot Report\\Wenying Deng \vspace{-1ex}}

\pretitle{\begin{flushright}\normalsize}
\posttitle{\par\end{flushright}}
\author{}
\date{}
\vspace{-10em}
\maketitle
\vspace{6em}


\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Formal Sections %%%%% %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\vspace{2em}
\section{{\bf Introduction}}
In recent years, kernel machine-based hypothesis tests (e.g. SKAT) for high-dimensional, nonlinear effects has seen widespread application in GWAS and gene-environment interaction studies. However, constructing a test for the interaction between groups of continuous features (for example, interaction between groups of air pollutants and multicategory nutrition intake) remains difficult in practice. The main challenges roots from (1) constructing an appropriate main-effect kernel that induce unbiased estimator for the null model, and (2) constructing an appropriate interaction-effect kernel describing only the effect of between-groups interaction, which is necessary for building a valid test statistic. Recently, \citep{liu_robust_2017} addressed challenge (1) by proposing CVEK, an ensemble-based estimator that adaptively learn the form of the main-effect kernel from data, and constructed an companion variance component test. While interesting, the null distribution of CVEK is constructed using asymptotic approximation, and requires the interaction-kernel to be fixed a priori, therefore calling into question the validity of the test in limited sample, and prevents practitioners from deploying flexible learning scheme for the interaction kernel. In this work, we seek to address these shortcomings by proposing a bootstrap test for CVEK. As we will illustrate through simulation, comparing to other popular nonlinear interaction tests, our test has robust Type I error and good power in small sample size and under a wide range of data-generation mechanisms.
\section{{\bf Method}}
\subsection{\textbf{CVEK}}
\setcounter{equation}{0}
\renewcommand{\theequation}{2.1.\arabic{equation}}
\subsubsection{{Main Algorithm}}
We study the problem of constructing a hypothesis test for an unknown data-generating function $h: \mathbb{R}^p\rightarrow \mathbb{R}$, when $h$ is estimated with a black-box algorithm (specifically, Gaussian Process regression) from $n$ observations $\{y_i, \bx_i\}_{i=1}^n$. We need a kernel estimation strategy \citep{liu_robust_2017} that is \textsl{flexible} so that it does not underfit under the null, yet \textsl{stable} so that it does not overfit under the alternative. To this end, we propose estimating $h$ using the ensemble of a library of fixed base kernels $\{k_d\}_{d=1}^D$:
\begin{align}
\hat{h}(\bx)=\sum_{d=1}^D u_d\hat{h}_d(\bx), \quad \bu \in \Delta=\{\bu | \bu \geq 0, \parallel \bu \parallel_1=1\}
\end{align}
where $\hat{h}_d$ is the kernel predictor generated by $d^{th}$ base kernel $k_d$.\\
To be more specific, for each given basis kernel $\{k_d\}_{d=1}^D$, we first estimate $\hat{\bh}_d=\bK_d(\bK_d+\hat{\lambda}_d\bI)^{-1}\by$, the prediction based on $d^{th}$ kernel, where the tuning parameter $\hat{\lambda}_d$ is selected by minimizing one of the four objective functions given in section 1.2. Denote the estimated error for $d^{th}$ kernel as $\hat{\epsilon}_d$ and $\bA_{d,\lambda}=\bK_d(\bK_d+\lambda\bI)^{-1}$. After that, we introduce two ways to ensemble these $D$ kernel predictors $\{\hat{h}_d\}_{d=1}^D$ in section 1.3. Then we get the ensemble matrix $\hat{\bA}$ and can estimate the ensemble kernel matrix $\hat{\bK}$ by solving:
\begin{align*}
\hat{\bK}(\hat{\bK}+\lambda \bI)^{-1}=\hat{\bA}
\end{align*}
Specifically, if we denote $\bU_A$ and $\{\delta_{A,k}\}_{k=1}^n$ the eigenvector and eigenvalues of $\hat{\bA}$, then $\hat{\bK}$ adopts the form:
\begin{align*}
\hat{\bK}=\bU_A diag(\frac{\delta_{A,k}}{1-\delta_{A,k}})\bU_A^T
\end{align*}
For example, a complete summary of the proposed procedure using LooCV to choose tuning parameter and cross-validation to ensemble kernel predictors is available in Algorithm 1.

%\subsubsection{Cross Validated Ensemble Kernel}

\begin{algorithm}
\caption{Cross Validated Ensemble Kernel (CVEK)} 
\label{alg:cvke}
\begin{algorithmic}[1]
\Procedure{CVEK}{}\newline
\textbf{Input:} A library of kernels $\{k_d\}_{d=1}^D$, Data $(\by, \bx)$\newline
\textbf{Output:} Ensemble Kernel Matrix $\widehat{\bK}$\newline
$\#$ \texttt{Stage 1: Estimate $\lambda$ and CV error for each kernel}
\For{$d = 1$ to $D$}
\State{$\bK_d = \bK_d/tr(\bK_d)$}
\State{$\widehat{\lambda}_d = \mbox{\textit{argmin}} \; 
\texttt{LOOCV}\Big(\lambda | \bK_d \Big)$} 
\State{$\widehat{\epsilon}_d = 
\texttt{CV}\Big(\widehat{\lambda}_d | \bK_d \Big)$}
\EndFor 
\newline
$\#$ \texttt{Stage 2: Estimate ensemble weights $\bu_{D \times 1} = \{u_1, \dots, u_D\}$}
\State{
$\widehat{\bu} = \underset{\bu \in \Delta}{argmin} \; ||\sum_{d=1}^D u_d \widehat{\epsilon}_d||^2 \qquad 
\mbox{where} \quad
\Delta = \{\bu| \bu \geq 0, ||\bu||_1 = 1 \}$}
\newline
$\#$ \texttt{Stage 3: Assemble the ensemble kernel matrix $\widehat{\bK}_{ens}$}
\State{$\widehat{\bA} = \sum_{d=1}^D 
\widehat{\mu}_d\bA_{\widehat{\lambda}_d, k_d}$}
\State{$\bU_A, \bdelta_{A} = 
\texttt{spectral\_decomp}(\widehat{\bA})$}
\State{$\lambda_{\bK} = min \Big(1,  
(\sum_{k=1}^n \frac{\delta_{A, k}}{1 - \delta_{A,k}})^{-1}, 
min \big(\{\widehat{\lambda}_d\}_{d=1}^D \big)
\Big)$}
\State{$\widehat{\bK} = 
\lambda_{\bK} * 
\widehat{\bU}_A \; 
diag\Big( \frac{\delta_{A,k}}{1 - \delta_{A,k}} \Big) \;
\widehat{\bU}_A^T$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{{Tuning Parameter Selection}}
Models may provide a good fit to the training data, but it will not fit sufficiently well to the test data. Tuning parameter could be chosen to address this problem. Here we define four objective functions in terms of tuning parameter $\lambda \in \Lambda$ to be minimized. Denote
\begin{align}
\bA_\lambda=\bK(\bX, \bX)[\bK(\bX, \bX)+\lambda \bI]^{-1}
\end{align}
In this way, $tr(\bA_\lambda)$ is the effective number of model parameters, excluding $\mu$ and $\sigma^2$. It decreases monotonically with $\lambda>0$. Additionally, we denote $\by^\star$ as the centered $\by$: $\by^\star=\by-\hat{\bmu}$, where $\hat{\mu}=\frac{1}{n}\sum_{i=1}^ny_i$.

\paragraph{Cross-Validation: K Fold and Loo}\mbox{}\\
Cross validation is probably the simplest and most widely used method for estimating prediction error. Suppose we do a $K$-fold cross-validation, which partitions observations into $K$ groups, $\kappa(1),...,\kappa(K)$, and calculates $\bA_\lambda$ $K$ times, each time leaving out group $\kappa(i)$, to get $\bA_\lambda^{-\kappa(1)}, \bA_\lambda^{-\kappa(2)}$, etc. For $\bA_\lambda^{-\kappa(i)}$, cross-validated residuals are calculated on the observations in $\kappa(i)$, which did not contribute to estimating $\bA$. The objective function estimated prediction error and is the sum of the squared cross-validated residuals:
\begin{align}
\lambda_{K-CV}=\underset{\lambda \in \Lambda}{argmin}\;\Big\{log\sum_{i=1}^K[\by_{\kappa(i)}^\star-\bA_\lambda^{-\kappa(i)}\by_{\kappa(i)}^\star]^T[\by_{\kappa(i)}^\star-\bA_\lambda^{-\kappa(i)}\by_{\kappa(i)}^\star]\Big\}
\end{align}
LooCV is the situation when $K=n$. In this case, we can write our objective function as \citep{golub_generalized_1979}:
\begin{align}
\lambda_{n-CV}=\underset{\lambda \in \Delta}{argmin}\;\Big\{log\;\by^{\star T}[\bI_n-diag(\bA_\lambda)-\frac{1}{n}\bI_n]^{-1}(\bI_n-\bA_\lambda)^2[\bI_n-diag(\bA_\lambda)-\frac{1}{n}\bI_n]^{-1}\by^\star \Big\}
\end{align}

The value $K$ influences bias and variance of cross-validation. With $K=n$, the cross-validation estimator is approximately unbiased for the true (expected) prediction error because it almost use all the data in each training set. Therefore, it can have high variance because $n$ training sets are so similar to one another. Additionally, the computational burden is also considerable, requiring $n$ applications of the learning method. On the other hand, with larger $K$ such as 5 \citep{hastie_elements_2009} or 10, cross-validation will have lower variance, but making bias a problem.

\paragraph{AIC and Small Sample Correction}\mbox{}\\
Based on the idea of "model fit + model complexity", Akaike's Information Criterion (AIC) \citep{akaike_information_1998} choose $\lambda$ by minimizing,
\begin{align*}
AIC=&2(p+2)-2log(\hat{L})\\
=&2(p+2)-2[-\frac{n}{2}log (\hat{\sigma}^2)-\frac{n}{2}log (2\pi)-\frac{1}{2\hat{\sigma}^2}\by^{\star T}(\bI_n-\bA_\lambda)^2\by^\star]\\
=&2(p+2)+n log[\frac{1}{n}\by^{\star T}(\bI_n-\bA_\lambda)^2\by^\star]+n+n log(2\pi)
\end{align*}
Drop the constant $n$ and divide it by $n$, we obtain our objective function:
\begin{align}
\lambda_{AIC}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\; \by^{\star T}(\bI_n-\bA_\lambda)^2\by^\star+\frac{2[tr(\bA_\lambda)+2)]}{n}\Big\}
\end{align}
When $n$ is small, extreme overfitting is possible, giving small bias/ large variance estimates. The small-sample correction of AIC \citep{hurvich_regression_1989, hurvich_clifford_m._smoothing_2002} is derived by minimizing minus 2 times expected log likelihood, where we plug in $\bA_\lambda$ and $\hat{\sigma}^2$. In this case, we obtain our small-sample size objective function AICc:
\begin{align}
\lambda_{AICc}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\; \by^{\star T}(\bI_n-\bA_\lambda)^2\by^\star+\frac{2[tr(\bA_\lambda)+2)]}{n-tr(\bA_\lambda)-3}\Big\}
\end{align}
Compare (2.1.5) and (2.1.6), it is easy to tell that AICc considers more of the model complexity since $\frac{n}{n-tr(\bA_\lambda)-3}>1$. It makes sense intuitively because it need to shrink more to prevent small bias/ large variance estimates.

\paragraph{GCV and Small Sample Correction}\mbox{}\\
In (2.1.4), if we approximate each $A_{\lambda[ii]}$ with their mean $\frac{tr(\bA_\lambda)}{n}$, in a sense that we give equal weight to all observations. We get our GCV objective function:
\begin{align}
\lambda_{GCV}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\; \by^{\star T}(\bI_n-\bA_\lambda)^2\by^\star-2log[1-\frac{tr(\bA_\lambda)}{n}-\frac{1}{n}]\Big\}
\end{align}
The "$-\frac{1}{n}$" terms in (2.1.7) is because GCV counts $\mu$ as part of model complexity, but not $\sigma^2$. This motivates the proposed small-sample correction to GCV \citep{boonstra_small-sample_2015}, which does count $\sigma^2$ as a parameter:
\begin{align}
\lambda_{GCVc}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\; \by^{\star T}(\bI_n-\bA_\lambda)^2\by^\star-2log[1-\frac{tr(\bA_\lambda)}{n}-\frac{2}{n}]_+\Big\}
\end{align}
Under this situation, perfect fit of the observations to the predictions, given by $\lambda=0$, cannot occur.


\paragraph{GMPML-based Selection}\mbox{}\\
Suppose we relate $Y$ and $\bX$ by a linear model, $Y=\mu+\phi(\bX)^T\bbeta+\sigma \epsilon$, with $\epsilon \sim N(0, 1)$, where $\phi(\bx)$ is a function mapping a $D$-dimensional input vector $\bx$ into an $p$-dimensional feature space. If we assume $\bbeta$ are jointly and independently normal with mean zero and variance $\sigma^2/\lambda$, the penalty term matches the negative normal log-density, up to a normalizing constant not depending on $\bbeta$:
\begin{align*}
p_\lambda(\bbeta, \sigma^2)=\frac{\lambda}{2\sigma^2}\bbeta^T\bbeta-\frac{p}{2}log(\lambda)+\frac{p}{2}log(\sigma^2)
\end{align*}
One can consider a marginal likelihood, where $\lambda$ is interpreted as the variance component of a mixed-effects model:
\begin{align*}
m(\lambda, \sigma^2)=&log \int_{\bbeta} exp\{ l(\bbeta, \sigma^2)-p_\lambda(\bbeta, \sigma^2)\}d\bbeta \\=&-\frac{1}{2\sigma^2}\by^{\star T}(\bI_n-\bA_\lambda)\by^\star-\frac{n}{2}log(\sigma^2)+\frac{1}{2}log \mid \bI_n-\bA_\lambda \mid
\end{align*}
From this, $\by^\star |\lambda, \sigma^2$ is multivariate normal with mean $\mathbf{0}_n$ and covariance $\sigma^2(\bI_n-\bA_\lambda)^{-1}$. The maximum profile marginal likelihood (MPML) estimate, originally proposed for smoothing spline \citep{wecker_signal_1983}, profiles $m(\lambda, \sigma^2)$ over $\sigma^2$, replacing each instance with $\hat{\sigma}_\lambda^2=\by^{\star T}(\bI_n-\bA_\lambda)\by^\star/n$, and maximized the "concentrated" log-likelihood, $m(\lambda, \hat{\sigma}_\lambda^2)$:
\begin{align*}
\lambda_{MPML}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\; \by^{\star T}(\bI_n-\bA_\lambda)\by^\star-\frac{1}{n}log \mid \bI_n-\bA_\lambda \mid \Big\}
\end{align*}
Closely related is the generalized/ restricted MPML \citep{harville_maximum_1977, wahba_comparison_1985}, which adjusts the penalty to account for estimation of regression parameter $\beta_0$ that is not marginalized, resulting in one degree of freedom:
\begin{align}
\lambda_{GMPML}=\underset{\lambda \in \Lambda}{argmin}\Big\{log\; \by^{\star T}(\bI_n-\bA_\lambda)\by^\star-\frac{1}{n-1}log \mid \bI_n-\bA_\lambda \mid \Big\}
\end{align}


\paragraph{Discussion on AIC, GCV and GMPML}\mbox{}\\
It's interesting to compare the $\lambda$'s selected from AIC, GCV, or GMPML, since AIC comes from Kullback-Leibler divergence, GCV from cross-validation and GMPML from likelihood. To do this, first we need to introduce some notations.\\
If we denote $\bU_K$ and $\{\eta_{K, j}\}_{j=1}^n$ the eigenvector and eigenvalues of $\bK$, then $\bA_\lambda$ adopts the form:
\begin{align*}
\bA_\lambda=\bU_K diag(\frac{\eta_{K, j}}{\eta_{K, j}+\lambda})\bU_K^T=\bU_K \bD_{K,\lambda}\bU_K^T
\end{align*}

\begin{comment}
As we can see from the two objective functions, $\lambda_{GCV}$ and $\lambda_{GMPML}$ share the same "model fit" term $log\; \by^{\star T}(\bI_n-\bA_\lambda)\by^\star$. Their difference focuses on "model complexity" terms, which is maximizing $2log[1-\frac{tr(\bA_\lambda)}{n}-\frac{1}{n}]$ for $\lambda_{GCV}$ and $\frac{1}{n-1}log \mid \bI_n-\bA_\lambda \mid$ for $\lambda_{GMPML}$. Up to a constant not depending on $\lambda$, the difference boils down to,
\begin{align}
&GCV: \; log[tr(\bI_n-\bA_\lambda)]=log[\sum_{j=1}^n\frac{\lambda}{\eta_{K, j}+\lambda}]\\
&GMPML: \; log\mid \bI_n-\bA_\lambda\mid=log[\prod_{j=1}^n\frac{\lambda}{\eta_{K, j}+\lambda}]
\end{align}

Take derivative of these two equations with respect to $\lambda$,
\begin{align*}
\frac{\partial }{\partial \lambda}log[tr(\bI_n-\bA_\lambda)]&=\frac{\sum_{j=1}^n\frac{\eta_{K, j}}{(\eta_{K, j}+\lambda)^2}}{\sum_{j=1}^n\frac{\lambda}{\eta_{K, j}+\lambda}}\\
\frac{\partial }{\partial \lambda}log\mid \bI_n-\bA_\lambda\mid&=\sum_{j=1}^n\frac{\eta_{K, j}}{\lambda(\eta_{K, j}+\lambda)}
\end{align*}
\end{comment}

Denote the objective functions in (2.1.5), (2.1.7) and (2.1.9) as $f_{AIC}$, $f_{GCV}$ and $f_{GMPML}$, and calculate the derivatives of them with respect to $\lambda$ respectively,
\begin{align}
\frac{\partial f_{AIC}}{\partial \lambda}&=\frac{2tr\Big[\bU_K^T\by^\star \by^{\star T}\bU_K(\bD_{K, \lambda}-1)\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{tr[\bU_K^T\by^\star \by^{\star T}\bU_K(\bI_n-\bD_{K, \lambda})^2]}+\frac{2tr\Big(\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big)}{n}\\
\frac{\partial f_{GCV}}{\partial \lambda}&=\frac{2tr\Big[\bU_K^T\by^\star \by^{\star T}\bU_K(\bD_{K, \lambda}-1)\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{tr[\bU_K^T\by^\star \by^{\star T}\bU_K(\bI_n-\bD_{K, \lambda})^2]}+\frac{2tr\Big(\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big)}{n-tr(\bA_\lambda)-1}\\
\frac{\partial f_{GMPML}}{\partial \lambda}&=\frac{-tr\Big[\bU_K^T\by^\star \by^{\star T}\bU_K \frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{tr[\bU_K^T\by^\star \by^{\star T}\bU_K(\bI_n-\bD_{K, \lambda})]}+\frac{tr\Big[(\bI_n-\bD_{K, \lambda})^{-1}\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{n-1}
\end{align}

Notice for $j^{th}$ element of diagonal vector of $\bD_{K, \lambda}$, its derivative with respect to $\lambda$ is negative,
\begin{align*}
\frac{\partial }{\partial \lambda}\Big[\frac{\eta_{K, j}}{\eta_{K, j}+\lambda}\Big]=-\frac{\eta_{K, j}}{(\eta_{K, j}+\lambda)^2}<0, \quad for\; j=1,\;2,...,\;n
\end{align*}
Further notice that the difference between (2.1.10) and (2.1.11) focuses on the second terms, both of which are increasing function of $\lambda$.
\begin{align*}
\frac{2tr\Big(\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big)}{n-tr(\bA_\lambda)-1}<\frac{2tr\Big(\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big)}{n}
\end{align*}
Therefore, when $\frac{\partial f_{AIC}}{\partial \lambda}=0$, $\frac{\partial f_{GCV}}{\partial \lambda}<0$, which means $\lambda_{AIC}<\lambda_{GCV}$.\\

\begin{comment}
On the other hand, we compare (2.1.12) and (2.1.13), and notice that,
\begin{align*}
\frac{-tr\Big[\bU_K^T\by^\star \by^{\star T}\bU_K\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{tr[\bU_K^T\by^\star \by^{\star T}\bU_K(\bI_n-\bD_{K, \lambda})^2]}>\frac{-tr\Big[\bU_K^T\by^\star \by^{\star T}\bU_K \frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{tr[\bU_K^T\by^\star \by^{\star T}\bU_K(\bI_n-\bD_{K, \lambda})]}, \quad \frac{2tr\Big[\bU_K^T\by^\star \by^{\star T}\bU_K\bD_{K, \lambda}\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{tr[\bU_K^T\by^\star \by^{\star T}\bU_K(\bI_n-\bD_{K, \lambda})^2]}<0
\end{align*}

and if most of $\eta_{K, j}'s>\lambda$, then,
\begin{align*}
\frac{2tr\Big(\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big)}{n}<\frac{tr\Big[(\bI_n-\bD_{K, \lambda})^{-1}\frac{\partial \bD_{K, \lambda}}{\partial \lambda}\Big]}{n-1}
\end{align*}

In this case, we obtain $\frac{\partial f_{AIC}}{\partial \lambda}<\frac{\partial f_{GMPML}}{\partial \lambda}$, which means $\lambda_{GMPML}<\lambda_{AIC}$.
\end{comment}

In terms of GCV and GMPML, probably we need the relative size of $\eta_{K, j}$'s and $\lambda$ to make a deterministic conclusion. To be more specific, we need to prove,
\begin{align*}
\frac{\partial }{\partial \lambda}\Big[log\; \by^{\star T}(\bI_n-\bA_\lambda)^2\by^\star\Big]>\frac{\partial }{\partial \lambda}\Big[log\; \by^{\star T}(\bI_n-\bA_\lambda)\by^\star\Big]
\end{align*}
And then, up to a constant not depending on $\lambda$, the difference boils down to,
\begin{align}
&GCV: \; log[tr(\bI_n-\bA_\lambda)]=log[\sum_{j=1}^n\frac{\lambda}{\eta_{K, j}+\lambda}]\\
&GMPML: \; log\mid \bI_n-\bA_\lambda\mid=log[\prod_{j=1}^n\frac{\lambda}{\eta_{K, j}+\lambda}]
\end{align}

Take derivative of these two equations with respect to $\lambda$,
\begin{align*}
\frac{\partial }{\partial \lambda}log[tr(\bI_n-\bA_\lambda)]&=\frac{\sum_{j=1}^n\frac{\eta_{K, j}}{(\eta_{K, j}+\lambda)^2}}{\sum_{j=1}^n\frac{\lambda}{\eta_{K, j}+\lambda}}\\
\frac{\partial }{\partial \lambda}log\mid \bI_n-\bA_\lambda\mid&=\sum_{j=1}^n\frac{\eta_{K, j}}{\lambda(\eta_{K, j}+\lambda)}
\end{align*}

Since,
\begin{align*}
\sum_{j=1}^n\frac{\frac{\eta_{K, j}}{(\eta_{K, j}+\lambda)^2}}{\sum_{j=1}^n\frac{\lambda}{\eta_{K, j}+\lambda}}\leq \sum_{j=1}^n\frac{\frac{\eta_{K, j}}{(\eta_{K, j}+\lambda)^2}}{\frac{\lambda}{\eta_{K, j}+\lambda}}=\sum_{j=1}^n\frac{\eta_{K, j}}{\lambda(\eta_{K, j}+\lambda)}
\end{align*}
which gives us,
\begin{align*}
-\frac{2}{n}log\mid \bI_n-\bA_\lambda\mid \geq -\frac{1}{n}log[tr(\bI_n-\bA_\lambda)]
\end{align*}
Therefore, we obtain $\frac{\partial f_{GMPML}}{\partial \lambda}<\frac{\partial f_{GCV}}{\partial \lambda}$. When $\frac{\partial f_{GCV}}{\partial \lambda}=0$, $\frac{\partial f_{GMPML}}{\partial \lambda}<0$, which means $\lambda_{GCV}<\lambda_{GMPML}$.



\subsubsection{{Ensemble Strategy}}

\paragraph{Empirical Risk Minimization}\mbox{}\\
After obtaining the estimated errors $\{\hat{\epsilon}_d\}_{d=1}^D$, we estimate the ensemble weights $\bu=\{u_d\}_{d=1}^D$ such that it minimizes the overall error \citep{liu_robust_2017}:
\begin{align*}
\hat{\bu}=\underset{\bu \in \Delta}{argmin}\parallel \sum_{d=1}^Du_d\hat{\epsilon}_d\parallel^2 \quad where\; \Delta=\{\bu | \bu \geq 0, \parallel \bu \parallel_1=1\}
\end{align*}
Then produce the final ensemble prediction:
\begin{align*}
\hat{\bh}=\sum_{d=1}^D \hat{u}_d \bh_d=\sum_{d=1}^D \hat{u}_d \bA_{d,\hat{\lambda}_d}\by=\hat{\bA}\by
\end{align*}
where $\hat{\bA}=\sum_{d=1}^D \hat{u}_d \bA_{d,\hat{\lambda}_d}$ is the ensemble matrix.

\paragraph{Simple Averaging}\mbox{}\\
Motivated by existing literature in omnibus kernel \citep{zhan_fast_2017}, we propose another way to obtain the ensemble matrix by simply choosing unsupervised weights $u_d=1/D$ for $d=1,2,...D$.

\subsubsection{{Kernel Choice}}
In this section we give introductions to some commonly-used covariance functions, including two stationary covariance functions, Squared Exponential Covariance Function and The Mat$\acute{e}$rn Class of Covariance Functions, as well as non-stationary covariance functions, Polynomial Covariance Functions and Neural Network Covariance Function.
\paragraph{Squared Exponential Covariance Function}\mbox{}\\
The \textsl{squared exponential} (SE) covariance function has the form
\begin{align*}
k_{SE}(r)=exp\Big(-\frac{r^2}{2l^2}\Big)
\end{align*}
with $r=|\bx-\bx'|$ and parameter $l$ defining the \textsl{characteristic length-scale}.This covariance function is infinitely differentiable, which means that the GP with this covariance function has mean square derivatives of all orders, and is thus very smooth. The spectral density of SE covariance function is $S(s)=(2\pi l^2)^{D/2}exp(-2\pi^2l^2s^2)$. \citep{stein_interpolation_1999} argues that such strong smoothness assumptions are unrealistic for modeling many physical processes, and recommends the Mat$\acute{e}$rn class. However, the squared exponential is probably the most widely-used kernel within the kernel machines field. The SE kernel is \textsl{infinitely divisible} in that $(k(r))^t$ is a valid kernel for all $t>0$; the effect of raising $k$ to the power of $t$ is simply to rescale $l$.

\paragraph{The Mat$\acute{\be}$rn Class of Covariance Functions}\mbox{}\\
The Mat$\acute{e}$rn class of covariance functions is given by
\begin{align*}
k_{Matern}(r)=\frac{2^{1-\nu}}{\Gamma(\nu)}\Big(\frac{\sqrt{2\nu r}}{l}\Big)^\nu K_\nu \Big(\frac{\sqrt{2\nu r}}{l}\Big)
\end{align*}
with positive parameters $\nu$ and $l$, where $K_\nu$ is a modified Bessel function \citep{abramowitz_handbook_1974}. This covariance function has a spectral density
\begin{align*}
S(s)=\frac{2^D\pi^{D/2}\Gamma(\nu+D/2)(2\nu)^\nu}{\Gamma(\nu)l^{2\nu}}\Big(\frac{2\nu}{l^2}+4\pi^2s^2\Big)^{-(\nu+D/2)}
\end{align*}
in D dimensions. For the Mat$\acute{e}$rn class the process $f(\bx)$ is $k$-times MS differentiable is and only if $\nu>k$. The Mat$\acute{e}$rn covariance functions become especially simple when $\nu$ is half-integer: $\nu=p+1/2$, where $p$ is a non-negative integer. In this case the covariance function is a product of an exponential and a polynomial of order $p$, the general expression can be derived from \citep{abramowitz_handbook_1974}, giving
\begin{align*}
k_{\nu=p+1/2}(r)=exp\Big(-\frac{\sqrt{2\nu r}}{l}\Big)\frac{\Gamma(p+1)}{\Gamma(2p+1)}\sum_{i=0}^p \frac{(p+i)!}{i!(p-i)!}\Big(\frac{\sqrt{8\nu r}}{l}\Big)^{p-i}
\end{align*}
It is possible that the most interesting cases for machine learning are $\nu=3/2$ and $\nu=5/2$, since for $\nu=1/2$ the process becomes very rough and for $\nu \geq 7/2$, in the absence of explicit prior knowledge about the existence of higher order derivatives, it is probably very hard from finite noisy training examples to distinguish between $\nu \geq 7/2$.


\paragraph{Polynomial Covariance Functions}\mbox{}\\
It is also interesting to show an explicit feature space construction for the polynomial covariance function. We consider the homogeneous polynomial case as the inhomogeneous case can simply be obtained by considering $\bx$ to be extended by concatenating a constant. We write
\begin{align*}
k(\bx, \bx')=&(\bx \cdot \bx')^p=(\sum_{d=1}^D x_d x'_d)^p=(\sum_{d_1=1}^D x_{d_1} x'_{d_1})...(\sum_{d_p=1}^D x_{d_p} x'_{d_p})\\
=&\sum_{d_1=1}^D...\sum_{d_p=1}^D(x_{d_1}...x_{d_p})(x'_{d_1}...x'_{d_p})\overset{\Delta}=\phi(\bx)\cdot \phi(\bx')
\end{align*}
Notice that this sum apparently contains $D^p$ terms but in fact it is less than this as the order of the indices in the monomial $x_{d_1}...x_{d_p}$ is unimportant, e.g. for $p=2$, $x_1x_2$ and $x_2x_1$ are the same monomial. We can remove the redundancy by defining a vector $\bm$ whose entry $m_d$ specifies the number of times index $d$ appears in the monomial, under the constraint that $\sum_{i=1}^D m_i=p$. Thus $\phi_\bm(\bx)$, the feature corresponding to vector $\bm$ is proportional to the monomial $x_1^{m_1}...x_D^{m_D}$. The degeneracy of $\phi_\bm(\bx)$ is $\frac{p!}{m_1!...m_D!}$, giving the feature map
\begin{align*}
\phi_\bm(\bx)=\sqrt{\frac{p!}{m_1!...m_D!}}x_1^{m_1}...x_D^{m_D}
\end{align*}
For example, for $p=2$ in $D=2$, we have $\phi(\bx)=(x_1^2, x_2^2, \sqrt{2}x_1x_2)^T$.

\paragraph{Neural Network Covariance Function}\mbox{}\\
Consider a network which takes an input $\bx$, has one hidden layer with $N_H$ units and then linearly combines the outputs of the hidden units with a bias $b$ to obtain $f(\bx)$. The mapping can be written
\begin{align*}
f(\bx)=b+\sum_{j=1}^{N_H}v_j h(\bx;\bu_j)
\end{align*}
where the $v_j$'s are the hidden-to-output weights and $h(\bx;\bu)$ is the hidden unit transfer function (which we shall assume is bounded) which depends on the input-to-hidden weights $\bu$. Let $b$ and the $v$'s have independent zero-mean distributions of variance $\sigma_b^2$ and $\sigma_v^2$, respectively, and let the weights $\bu_j$ for each hidden unit be independently and identically distributed. Denoting all weights by $\bw$, we obtain
\begin{align}
\mathbb{E}_\bw[f(\bx)]=&0\nonumber \\
\mathbb{E}_\bw[f(\bx)f(\bx')]=&\sigma_b^2+\sum_j \sigma_v^2 \mathbb{E}_\bu[h(\bx; \bu_j)h(\bx'; \bu_j)] \\
=&\sigma_b^2+N_H\sigma_v^2 \mathbb{E}_\bu[h(\bx; \bu)h(\bx'; \bu)]
\end{align}
where (2.1.16) follows because all of the hidden units are identically distributed. The sum in (2.1.15) is over $N_H$ identically and independently distributed random variables. As the transfer function is bounded, all moments of the distribution will be bounded and hence the central limit theorem can be applied, showing that the stochastic process will converge to a Gaussian process in the limit as $N_H \rightarrow \infty$.\\
By evaluating $\mathbb{E}_\bu[h(\bx; \bu)h(\bx'; \bu)]$ we can obtain the covariance function of the neutral network. For example if we choose the error function $h(z)=erf(z)=2/\sqrt{\pi}\int_0^z e^{-t^2}dt$ as the transfer function, let $h(\bx;\bu)=erf\Big(u_0+\sum_{j=1}^Du_jx_j\Big)$ and choose $\bu \sim N(0, \Sigma)$ then we obtain
\begin{align*}
k_{NN}(\bx, \bx')=\frac{2}{\pi}sin^{-1}\Big(\frac{2\tilde{\bx}^T\Sigma \tilde{\bx}'}{\sqrt{(1+2\tilde{\bx}^T\Sigma \tilde{\bx})(1+2\tilde{\bx}'^T\Sigma \tilde{\bx}')}}\Big)
\end{align*}
where $\tilde{\bx}=(1, x_1, ..., x_d)^T$ is an augmented input vector.


\subsection{{\textbf{Hypothesis Test}}}
\setcounter{equation}{0}
\renewcommand{\theequation}{2.2.\arabic{equation}}
\subsubsection{{Asymptotic Test}}
We use the classical variance component test \citep{lin_variance_1997} to construct a testing procedure for the hypothesis about Gaussian process function:
\begin{align}
H_0: h \in \Hsc_0.
\end{align}
We first translate above hypothesis into a hypothesis in terms of model parameters. The key of our approach is to assume that $h$ lies in a RKHS generated by a \textsl{garrote kernel function}$k_{\delta}(\bz, \bz')$ \citep{maity_powerful_2011}, which is constructed by including an extra \textsl{garrote parameter} $\delta$ to a given kernel function. When $\delta=0$, the garrote kernel function $k_0(\bx, \bx')=k_{\delta}(\bx, \bx')\mid _{\delta=0}$ generates exactly $\Hsc_0$, the space of functions under the null hypothesis. In order to adapt this general hypothesis to their hypothesis of interest, practitioners need only to specify the form of the garrote kernel so that $\Hsc_0$ corresponds to the null hypothesis. For example, if $k_{\delta}(\bx)=k(\delta * x_1, x_2,..., x_p), \delta=0$ corresponds to the null hypothesis $H_0: h(\bx)=h(x_2,...,x_p)$, i.e. the function $h(\bx)$ does not depend on $x_1$. As a result, the general hypothesis is equivalent to:
\begin{align}
H_0: \delta=0.
\end{align}
We now construct a test statistic $\hat{T}_0$ for (2.2.2) by noticing that the garrote parameter $\delta$ can be treated as a variance component parameter in the linear mixed model. This is because the Gaussian process under garrote kernel can be formulated into below LMM:
\begin{align*}
\by=\bmu+\bh+\bepsilon \quad where \quad \bh \sim N(\mathbf{0}, \tau \bK_\delta) \quad \bepsilon \sim N(\mathbf{0}, \sigma^2\bI)
\end{align*}
where $\bK_\delta$ is the kernel matrix generated by $k_{\delta}(\bz, \bz')$. Consequently, we can derive a variance component test for $H_0$ by calculating the square derivative of $L_{REML}$ with respect to $\delta$ under $H_0$ \citep{lin_variance_1997}:
\begin{align}
\hat{T}_0=\hat{\tau}*(\by-\hat{\bmu})^T\bV_0^{-1}[\partial \bK_0]\bV_0^{-1}(\by-\hat{\bmu})
\end{align}
where $\bV_0=\hat{\sigma}^2\bI+\hat{\tau}\bK_0$. In this expression, $\bK_0=\bK_\delta \mid_{\delta=0}$, and $\partial \bK_0$ is the null derivative kernel matrix whose $(i, j)^{th}$ entry is $\frac{\partial }{\partial \delta}k_\delta(\bx, \bx') \mid_{\delta=0}$.
As discussed previously, misspecifying the null kernel function $k_0$ negatively impacts the performance of the resulting hypothesis test. To better understand the mechanism at play, we express the test statistic $\hat{T}_0$ from (2.2.3) in terms of the model residual $\hat{\bepsilon}=\by-\hat{\bmu}-\hat{\bh}$:
\begin{align}
\hat{T}_0=(\frac{\hat{\tau}}{\hat{\sigma}^4})*\hat{\bepsilon}^T[\partial \bK_0]\hat{\bepsilon},
\end{align}
where we have used the fact $\bV_0^{-1}(\by-\hat{\bmu})=(\hat{\sigma}^2)^{-1}(\hat{\bepsilon})$ \citep{harville_maximum_1977}. As shown, the test statistic $\hat{T}_0$ is a scaled quadratic-form statistic that is a function of the model residual. If $k_0$ is too restrictive, model estimates will \textbf{underfit} the data even under the null hypothesis, introducing extraneous correlation among the $\hat{\epsilon}_i$'s, therefore leading to overestimated $\hat{T}_0$ and eventually underestimated p-value under under the null. In this case, the test procedure will frequently reject the null hypothesis (i.e. suggest the existence of nonlinear interaction) even when there is in fact no interaction, yielding an invalid test due to \textbf{inflated Type I error}. On the other hand, if $k_0$ is too flexible, model estimates will likely \textbf{overfit} the data in small samples, producing underestimated residuals, an underestimated test statistic, and overestimated p-values. In this case, the test procedure will too frequently fail to reject the null hypothesis (i.e. suggesting there is no interaction) when there is in fact interaction, yielding an insensitive test with \textbf{diminished power}.\\
The null distribution of $\hat{T}$ can be approximated using a scaled chi-square distribution $\kappa \chi_\nu^2$ using Satterthwaite method by matching the first two moments of $T$:
\begin{align*}
\kappa * \nu=E(T)=\hat{\tau} * tr(\bV_0\partial \bK_0)
 \quad 2*\kappa^2*\nu=Var(T)=\hat{\bI}_{\delta\delta}
\end{align*}
with solution:
\begin{align*}
\hat{\kappa}=\hat{\bI}_{\delta\delta}/[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)] \quad \hat{\nu}=[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)]^2/(2*\hat{\bI}_{\delta\theta})
\end{align*}
where $\hat{\bI}_{\delta\delta}=\bI_{\delta\delta}-\bI_{\delta\theta}^T\bI_{\theta\theta}^{-1}\bI_{\delta\theta}$ is the efficient information of $\delta$ under REML. $\bI_{\delta\delta}$, $\bI_{\theta\theta}$ and $\bI_{\delta\theta}$ are submatrices of the REML information matrix. 
Numerically more accurate, but computationally less efficient approximation methods are also available.\\
Finally, the p-value of this test is calculated by examining the tail probability of $\hat{\kappa} \chi_{\hat{\nu}}^2$:
\begin{align*}
p=P(\hat{\kappa} \chi_{\hat{\nu}}^2>\hat{T})=P(\chi_{\hat{\nu}}^2>\hat{T}/\hat{\kappa})
\end{align*}
A complete summary of the proposed testing procedure is available in Algorithm 2.\\
In light of the discussion about model misspecification in Introduction section, we highlight the fact that our proposed test (2.2.3) is robust against model misspecification under the alternative \citep{lin_variance_1997}, since the calculation of test statistics do not require detailed parametric assumption about $k_\delta$. However, the test is NOT robust against model misspecification under the null, since the expression of both test statistic $\hat{T}_0$ and the null distribution parameters $(\hat{\kappa}, \hat{\nu})$ still involve the kernel matrices generated by $k_0$ (see Algorithm 2). We address this problem by proposing a robust estimation procedure for the kernel matrices under the null.

\begin{algorithm}
\caption{Variance Component Test for $h \in \Hsc_0$} 
\label{alg:cvek}
\begin{algorithmic}[1]
\Procedure{VCT FOR INTERACTION}{}\newline
\textbf{Input:} Null Kernel Matrix $\bK_0$, Derivative Kernel Matrix $\partial \bK_0$, Data $(y, \bx)$\newline
\textbf{Output:} Hypothesis Test p-value $p$\newline
$\#$ \texttt{Stage 1: Estimate Null Model using REML}
\State{$(\hat{\bmu}, \hat{\tau}, \hat{\sigma}^2)=argmin L_{REML}(\bmu, \tau, \sigma^2|\bK_0)$}
\newline
$\#$ \texttt{Stage 2: Compute Test Statistic and Null Distribution Parameters}
\State{$\hat{T}_0=\hat{\tau}*(\by-\bX\hat{\bbeta})^T\bV_0^{-1}\partial \bK_0\bV_0^{-1}(\by-\bX\hat{\bbeta})$}
\State{$\hat{\kappa}=\hat{\bI}_{\delta\delta}/[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)]$, \quad $\hat{\nu}=[\hat{\tau}*tr(\bV_0^{-1}\partial \bK_0)]^2/(2*\hat{\bI}_{\delta\theta})$}
\newline
$\#$ \texttt{Stage 3: Compute p-value and reach conclusion}
\State{$p=P(\hat{\kappa} \chi_{\hat{\nu}}^2>\hat{T})=P(\chi_{\hat{\nu}}^2>\hat{T}/\hat{\kappa})$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\subsubsection{{Bootstrap Test}}
In practice, the sample size of the collected data is always small. To make valid inferences about a population from the sample, we need to perform resampling. Commonly used methods in small sample size are permutation tests and bootstrap.\\
\paragraph{Permutation Tests}\mbox{}\\
Permutation tests are very popular in genomic research. They are simple to compute where analytic approaches may be intractable, and can be exact where analytic results may be only approximate. However, the main limitation of permutation tests is that they are only applicable when the null hypothesis being tested specifies a suitable group of permutations under which the distribution of the data would be unaffected. To illustrate this idea, consider a test for interaction between the effects of the genetic polymorphism $G$ and a environmental exposure $E$ on an outcome $Y$. The null hypothesis is that the interaction term is zero \citep{buzkova_permutation_2011}.
\begin{align}
E[Y]=\beta_0+\beta_GG+\beta_EE
\end{align}
An alternative hypothesis of interest may be that,
\begin{align*}
E[Y]=\beta_0+\beta_GG+\beta_EE+\delta G*E
\end{align*}
For a valid permutation test of the hypothesis of no interaction, we would require a group of permutations that exactly preserves both $\beta_G$ and $\beta_E$ in (2.2.5), but also ensures $\delta=0$. In general it is impossible to construct such a group of permutations, as demonstrated by Edgington (1987, Chap. 6). If the permutations fix $G$ and $E$ they will not give $\delta=0$, and if they do not fix $G$ and $E$ they will not preserve the relationship between $G$ and $E$ and so will not preserve $\beta_G$ and $\beta_E$.

\paragraph{Bootstrap Test}\mbox{}\\
Instead we can use parametric bootstrap, which can give valid tests with moderate sample sizes and requires similar computational effort to a permutation test.\\
Testing in a regression model framework requires computing the distribution of the test statistic under sampling from the null-hypothesis model. A good approximation to the distribution of the test statistic under sampling from the true-hypothesis model is the distribution of the test statistic under sampling from the fitted null-hypothesis model. For instance, when testing (2.2.2), we first fit the model under the null,
\begin{align}
E(\by^\star)=\bK_0(\bK_0+\lambda \bI)^{-1}\by=\bA_0\by
\end{align}
and generate $\bY^\star$ for each individuals with a random noise, whose variance is also estimated. We then compute the test statistic for this simulated sample, and repeat this process $B$ times. The empirical distribution these provide is an estimate of the test statistic's distribution under the null. Correspondingly, p-values are calculated as the proportion of simulated test statistics that are most extreme than the observed value.\\
If the distribution of the test statistic depends smoothly on the regression parameter values, which is true in all standard examples, this ‘parametric bootstrap’ approach gives an asymptotically valid test(Davison \& Hinkley 1997, 4.2.3). Like the classical bootstrap, it samples from a distribution based on the observed data, but the simulations are from a fitted parametric model rather than the empirical distribution. To obtain a valid test, the fitted parametric model is chosen so that the null hypothesis is satisfied.
A complete summary of the proposed testing procedure is available in Algorithm 3.\\

\begin{algorithm}
\caption{Parametric Bootstrap Test} 
\label{alg:cvek}
\begin{algorithmic}[1]
\Procedure{Parametric Bootstrap Test}{}\newline
\textbf{Input:} Null Kernel Matrix $\bK_0$, Derivative Kernel Matrix $\partial \bK_0$, Data $(\by, \bx)$\newline
\textbf{Output:} Hypothesis Test p-value $p$\newline
$\#$ \texttt{Stage 1: Estimate Null Model using Gaussian Process Regression}
\State{$\hat{\bmu}=\bA_0\by, \quad \hat{\sigma}^2=\frac{\by^T(\bI_n-\bA_0)\by}{n-tr(\bA_0)}, \quad \hat{\tau}$}
\newline
$\#$ \texttt{Stage 2: Sample response from the fitted model obtain in Step 1}
\newline
$\#$ \texttt{and compute the test statistic based on fitting the alternative}
\newline
$\#$ \texttt{model, repeat for $B$ times}
\For{$b= 1$ to $B$}
\State{$\by^\star=\hat{\bmu}+\bepsilon,\quad \bepsilon \sim N(0, \hat{\sigma}^2)$}
\State{$\hat{T}_{0b}=\hat{\tau}*(\by^\star-\hat{\bmu})^T\bV_0^{-1}\partial \bK_0\bV_0^{-1}(\by^\star-\hat{\bmu})$} 
\EndFor 
\newline
$\#$ \texttt{Stage 3: Compute the test statistic for the original data, based}
\newline
$\#$ \texttt{on fitting the alternative hypothesis model}
\State{$\hat{T}_0=\hat{\tau}*(\by-\hat{\bmu})^T\bV_0^{-1}\partial \bK_0\bV_0^{-1}(\by-\hat{\bmu})$}
\newline
$\#$ \texttt{Stage 4: Compute p-value and reach conclusion}
\State{$p=\frac{1}{B}\sum_{b=1}^BI(\hat{T}_{0b}>\hat{T}_0)$}
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{{\bf Simulation}}
We evaluated the finite-sample performance of the proposed interaction test in a simulation study that is analogous to a real nutrition-environment interaction study. We generate two groups of input features $(\bx_{i, 1}, \bx_{i,2}) \in \mathbb{R}^{p_1} \times \mathbb{R}^{p_2}$ independently from standard Gaussian distribution, representing normalized data representing subject's level of exposure to $p_1$ environmental pollutants and the levels of a subject's intake of $p_2$ nutrients during the study. Throughout the simulation scenarios, we keep $n=100$, and $p_1=p_2=2$. We generate the outcome $y_i$ as:
\begin{align*}
y_i=h_1(\bx_{i, 1})+h_2(\bx_{i, 2})+\delta * h_1(\bx_{i, 1}) * h_2(\bx_{i, 2})+\epsilon_i
\end{align*}
where $h_1$, $h_2$ are sampled from RKHSs $\Hsc_1$, $\Hsc_2$, generated using a ground-truth kernel $k_{true}$. We standardize all sampled functions to have unit form, so that $\delta$ represents the strength of interaction relative to the main effect.\\
For each simulation scenario, we first generated data using $\delta$ and $k_{true}$ as above, then selected a $k_{model}$ to estimate the null model and obtain p-value using Algorithm 3. We repeated each scenario 200 times, and evaluate the test performance using the empirical probability $\hat{P}(p\leq 0.05)$ estimates the test's Type I error, and should be smaller or equal to the significance level 0.05. Under alternative hypothesis $H_a: \delta>0$, $\hat{P}(p\leq 0.05)$ estimates the test's power, and should ideally approach 1 quickly as the strength of interaction $\delta$ increases.\\
In this study, we varied $k_{true}$ to produce data-generating functions $h_\delta(\bx_{i, 1}, \bx_{i, 2})$ with different smoothness and complexity properties, and varied $k_{model}$ to reflect different common modeling strategies for the null model in addition to using CVEK. We then evaluated how these two aspects impact the hypothesis test's Type I error and power.

\paragraph{Data-generating Mechanism}\mbox{}\\
We consider 15 data-generating mechanisms:
\begin{enumerate}[1)]
\item Matern $\nu=1/2$ ($l=0.5, 1, 1.5$)
\item Matern $\nu=3/2$ ($l=0.5, 1, 1.5$)
\item Matern $\nu=5/2$ ($l=0.5, 1, 1.5$)
\item Polynomial ($p=1, 2, 3$)
\item RBF ($l=0.5, 1, 1.5$)
\end{enumerate}

\paragraph{Tuning Parameter}\mbox{}\\
Loop over the four methods: LooCV, AICc, GCVc and GMPML respectively.

\paragraph{Modeling Strategies}\mbox{}\\
We consider 5 kinds of kernel libraries:
\begin{enumerate}[1)]
\item True kernel only
\item 3 Polynomial kernels ($p=1, 2, 3$)
\item 3 RBF kernels ($l=0.6, 1, 2$)
\item 3 Polynomial kernels ($p=1, 2, 3$) and 3 RBF kernels ($l=0.6, 1, 2$)
\item 3 $l=1$ Matern kernels ($\nu=1/2, 3/2, 5/2$) and 3 RBF kernels ($l=0.6, 1, 2$)
\end{enumerate}

\paragraph{Ensemble Strategies}\mbox{}\\
Loop over the two methods: erm ensemble and simple averaging respectively.



\begin{figure}
\begin{center}
\includegraphics[width=1.1\columnwidth]{L1} 
\caption{Results of library 1}
\label{fig:res}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1.1\columnwidth]{L2} 
\caption{Results of library 2}
\label{fig:res}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1.1\columnwidth]{L3} 
\caption{Results of library 3}
\label{fig:res}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1.1\columnwidth]{L4} 
\caption{Results of library 4}
\label{fig:res}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=1.1\columnwidth]{L5} 
\caption{Results of library 5}
\label{fig:res}
\end{center}
\end{figure}


\newpage
\section{{\bf Conclusion}}
\paragraph{true kernel only}\mbox{}\\
\begin{enumerate}[1)]
\item If the data-generating function space $\Hsc$ is too flexible, model estimates will likely \textbf{overfit} the data even in the true kernel.
\item Empirically, loocv leads to \textbf{overfitting}.
\item Validated theory: $\lambda_{AIC}<\lambda_{GCV}$. 
%Also, the gap in model selection depends on rate of eigendecay.
\end{enumerate}

\paragraph{library of polynomials}\mbox{}\\
\begin{enumerate}[1)]
\item Models estimates are capable of achieving satisfying performance (\textbf{correct Type I error} and \textbf{decent power} compared to true kernel), but only when tuning parameter and ensemble method are chosen carefully.
\item loocv leads to \textbf{less power} but \textbf{better Type I error}.
\item In terms of ensemble strategy, average is better than erm.
\end{enumerate}



\clearpage
\bibliography{reference}


%----------------------------------------------------------------------------------------
\end{document}