\documentclass[11pt]{article}

% 
\usepackage{natbib}
\bibliographystyle{unsrtnat}

% change document font family to Palatino, and code font to Courier
\usepackage{mathpazo} % add possibly `sc` and `osf` options
\usepackage{eulervm}
\usepackage{courier}
%allow formula formatting
\usepackage{amsmath}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\usepackage{comment}
\usepackage{enumerate}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
%title positon
\usepackage{titling} %fix title
\setlength{\droptitle}{-6em}   % Move up the title 

\usepackage{verbatim}
% change page margin
\usepackage[margin=1 in]{geometry} 
\usepackage{subfigure}
%allow inserting multiple graphs
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{float}
%allow code chunks
\usepackage{listings}
\lstset{basicstyle=\footnotesize\ttfamily,breaklines=true}
\lstset{frame=lrbt,xleftmargin=\fboxsep, xrightmargin=-\fboxsep}

%declare sum int sign
\DeclareMathOperator*{\SumInt}{%
\mathchoice%
  {\ooalign{$\displaystyle\sum$\cr\hidewidth$\displaystyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.14\height}{\scalebox{.7}{$\textstyle\sum$}}\cr\hidewidth$\textstyle\int$\hidewidth\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
  {\ooalign{\raisebox{.2\height}{\scalebox{.6}{$\scriptstyle\sum$}}\cr$\scriptstyle\int$\cr}}
}

\usepackage{"./macro/GrandMacros"}
\usepackage{"./macro/Macro_BIO235"}
\begin{document}
\setlength{\parindent}{0pt}
%\setcounter{equation}{0}
%\renewcommand{\theequation}{1.1.\arabic{equation}}
%\paragraph{AIC and small sample correction}\mbox{}\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% TItle page with contents %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{ Appendix for CVEK-boot\\Wenying Deng \vspace{-1ex}}

\pretitle{\begin{flushright}\normalsize}
\posttitle{\par\end{flushright}}
\author{}
\date{}
\vspace{-10em}
\maketitle
\vspace{6em}


\tableofcontents
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%% Formal Sections %%%%% %%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\vspace{2em}
\setcounter{equation}{0}
\renewcommand{\theequation}{1.\arabic{equation}}
\section{{\bf Derivation of Derivative wrt $\lambda$}}
Corresponding to (2.2.3),
\begin{align*}
\by=\bmu+\bh+\bepsilon \quad where \quad \bh \sim N(\mathbf{0}, \tau \bK_\delta) \quad \bepsilon \sim N(\mathbf{0}, \sigma^2\bI)
\end{align*}
where $\bK_\delta$ is the kernel matrix generated by $k_{\delta}(\bz, \bz')$.\\
the general model may be expressed in matrix form as \citep{reiss_smoothing_2009}:
\begin{align}
\by=\bmu+\Phi(\bX)^T \bbeta+\bepsilon \quad where\; \Phi(\bX)^T \;is\; n\times p
\end{align}
where $\Phi(\bX)$ is the aggregation of columns $\phi(\bx)$ for all cases in the training set, and $\phi(\bx)$ is a function mapping a $D$-dimensional input vector $\bx$ into an $p$-dimensional feature space. This model is fitted by penalized least squares, i.e., our estimate is
\begin{align}
(\hat{\mu}, \hat{\bbeta})=\underset{\mu, \bbeta}{argmin} \; (\parallel \by-\bmu-\Phi(\bX)^T \bbeta \parallel^2+\lambda \bbeta^T\bbeta)
\end{align}
The development that follows depends on the following Assumptions:
\begin{enumerate}[1.]
%\item $\Phi(\bX)^T$ a full-rank $n\times p$ matrix, with $p+1\leq n$.
\item $\bf{1}^T$$\Phi(\bX)^T=\mathbf{0}$.
\item $\by$ is not in the column space of $\bf{1}$.
\end{enumerate}
where $\mathbf{1}$ is a $n\times 1$ vector.
\subsection{\textbf{Derivative of REML}}
As our choice of matrix notation suggests, model (1.1) can be seen as equivalent to a linear mixed model, in the following sense. The criterion in (1.2) is proportional to the log likelihood for the partly observed "data" $(\by, \bbeta)$ with respect to the unknowns $\bmu$ and $\bbeta$, i.e., the best linear unbiased prediction (BLUP) criterion, for the mixed model
\begin{align*}
\by|\bbeta \sim N(\bmu+\Phi(\bX)^T \bbeta, \sigma^2\bI), \quad \bbeta \sim N(0, \sigma/\lambda)
\end{align*}
Under this model, $Var(\by)=\sigma^2 \bV_\lambda$ where
\begin{align}
\bV_\lambda=\bI+\lambda^{-1}\Phi(\bX)^T\Phi(\bX)=\bI+\lambda^{-1}\bK
\end{align}
The mixed model formulation motivates treating $\lambda$ as a variance parameter to be estimated by maximizing the log likelihood
\begin{align*}
l(\mu, \lambda, \sigma|\by)=-\frac{1}{2}\Big[log\mid \sigma^2 \bV_\lambda \mid +(\by-\bmu)^T(\sigma^2 \bV_\lambda)^{-1}(\by-\bmu) \Big]
\end{align*}
Maximizing this log likelihood results in estimating $\sigma^2$ with a downward bias, which is removed if we instead maximize the restricted log likelihood
\begin{align}
l_R(\mu, \lambda, \sigma|\by)=-\frac{1}{2}\Big[log\mid \sigma^2 \bV_\lambda \mid +(\by-\bmu)^T(\sigma^2 \bV_\lambda)^{-1}(\by-\bmu)+log \mid \sigma^{-2}\mathbf{1}^T\bV_\lambda^{-1}\mathbf{1} \mid \Big]
\end{align}
We shall refer to the resulting estimate of $\lambda$ as the REML choice of the parameter.\\
For given $\mu$ and $\lambda$, the value of $\sigma^2$ maximizing the restricted log likelihood (1.4) is
\begin{align}
\hat{\sigma}_{\mu, \lambda}^2=(\by-\bmu)^T\bV_\lambda^{-1}(\by-\bmu)/(n-1)
\end{align}
substituting in this value and ignoring an additive constant leads to the profile restricted log likelihood
\begin{align}
l_R(\mu, \lambda|\by)=-\frac{1}{2}\Big[log\mid \bV_\lambda \mid +log \mid \mathbf{1}^T\bV_\lambda^{-1}\mathbf{1} \mid+(n-1)log\{(\by-\bmu)^T \bV_\lambda^{-1}(\by-\bmu)\}\Big]
\end{align}
For given $\lambda$, the value of $\mu$ maximizing this last expression is the generalized least square fit $\hat{\mu}_\lambda=(\mathbf{1}^T\bV_\lambda^{-1}\mathbf{1})^{-1}\mathbf{1}^T\bV_\lambda^{-1}\by$.\\
Using the readily verified equality $\bV_\lambda^{-1}=\bI-\bA_\lambda$, the following key facts about $\bP_\lambda$ can be shown to hold under Assumptions 1-2:
\begin{align}
\bP_\lambda=\bI-\bH_\lambda
\end{align}
where $\bH_\lambda$ is the hat matrix defined by $\hat{\by}=\bH_\lambda \by$ and given by
\begin{align}
\bH_\lambda=\mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T&+\bA_\lambda\\
\bV_\lambda^{-1}\mathbf{1}=\mathbf{1}\\
\bP_\lambda^k=\bV_\lambda^{-k}-\mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}&\mathbf{1}^T\;for\;k=1,\;2,...
\end{align}
Under Assumptions 1-2, repeated application of (1.9) gives $\by-\hat{\bmu}_\lambda=[\bI-\mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T]\by$, and hence
\begin{align}
(\by-\hat{\bmu}_\lambda)^T\bV_\lambda^{-1}(\by-\hat{\bmu}_\lambda)=\by^T\bP_\lambda \by
\end{align}
Substituting (1.11) into (1.6) yields the profile restricted log likelihood for $\lambda$ alone:
\begin{align}
l_R(\lambda|\by)=-\frac{1}{2}\Big[log\mid \bV_\lambda \mid +log \mid \mathbf{1}^T\bV_\lambda^{-1}\mathbf{1} \mid+(n-1)log(\by^T\bP_\lambda \by)\Big]
\end{align}
Setting the derivative of (1.12) with respect of $\lambda$ to zero will yield an equation for the REML estimate of $\lambda$. By (1.9) again, $log \mid \mathbf{1}^T\bV_\lambda^{-1}\mathbf{1} \mid=log \mid \mathbf{1}^T\mathbf{1} \mid$, which does not depend on $\lambda$, so the differentiation reduces to finding the derivatives of $log\mid \bV_\lambda \mid$ and $log(\by^T\bP_\lambda \by)$. To that end we shall need the (component-wise) derivatives of $\bV_\lambda$ and $\bP_\lambda$ with respect to $\lambda$; these can be shown to be:
\begin{align}
\frac{\partial \bV_\lambda}{\partial \lambda}=&\lambda^{-1}(\bI-\bV_\lambda)\\
\frac{\partial \bP_\lambda}{\partial \lambda}=&\lambda^{-1}(\bP_\lambda-\bP_\lambda^2)
\end{align}
A formula in \citep{lindstrom_newtonraphson_1988}(p. 1016), together with (1.13), leads to
\begin{align*}
\frac{\partial}{\partial \lambda}log\mid \bV_\lambda \mid=\lambda^{-1}tr(\bV_\lambda^{-1}-\bI)
\end{align*}
By (1.10), $tr(\bV_\lambda^{-1})=tr(\bP_\lambda)+tr[\bI-\mathbf{1}(\mathbf{1}^T\mathbf{1})^{-1}\mathbf{1}^T]=tr(\bP_\lambda)+1$, so we conclude that
\begin{align}
\frac{\partial}{\partial \lambda}log\mid \bV_\lambda \mid=\lambda^{-1}[tr(\bP_\lambda)-(n-1)]
\end{align}
By Assumption 2, $\by^T\bP_\lambda \by>0$. Thus, using (1.14), we obtain
\begin{align}
\frac{\partial}{\partial \lambda}log(\by^T\bP_\lambda \by)=\lambda^{-1}\Big[1-\frac{\by^T\bP_\lambda^2\by}{\by^T\bP_\lambda \by} \Big]
\end{align}
Under our Assumptions, the matrix
\begin{align*}
\bP_\lambda=\bV_\lambda^{-1}-\bV_\lambda^{-1}\mathbf{1}(\mathbf{1}^T\bV_\lambda^{-1}\mathbf{1})^{-1}\mathbf{1}^T\bV_\lambda^{-1}
\end{align*}
which plays a role in some treatments of mixed model theory, turns out to be important for both the REML and the GCV approach to choosing $\lambda$.\\
By (1.12), (1.15) and (1.16), we obtain
\begin{align}
\frac{\partial l_R(\lambda|\by)}{\partial \lambda}=\frac{1}{2\lambda}\Big[(n-1)\frac{\by^T\bP_\lambda^2\by}{\by^T\bP_\lambda \by}- tr(\bP_\lambda)\Big]
\end{align}
Thus by (1.7), (1.11) and (1.17), $\frac{\partial l_R(\lambda|\by)}{\partial \lambda}=0$ implies
\begin{align}
\frac{(\by-\hat{\bmu}_\lambda)^T\bV_\lambda^{-1}(\by-\hat{\bmu}_\lambda)}{n-1}=\frac{\by^T (\bI-\bH_\lambda)^2\by}{tr(\bI-\bH_\lambda)}
\end{align}
which is also
\begin{align}
\frac{\by^T\bP_\lambda \by}{n-1}=\frac{\by^T \bP_\lambda^2\by}{tr(\bP_\lambda)} \quad or\quad \frac{\by^T\bP_\lambda \by}{\by^T \bP_\lambda^2\by}=\frac{n-1}{tr(\bP_\lambda)}
\end{align}
where $\hat{\mu}_\lambda$ and $\bH_\lambda$ are the parameter estimate and hat matrix, respectively, obtained with smoothing parameter value $\lambda$. The left side of (1.18) is the REML estimate of $\sigma^2$ \citep{wahba_spline_1990}. The right side equals $\parallel \by-\hat{\by}\parallel ^2/[n-tr(\bH_\lambda)]$, an estimate of $\sigma^2$ based on viewing $tr(\bH_\lambda)$ as the degrees of freedom of the smoother \citep{pawitan_all_2001}(p. 487) and \citep{lee_generalized_2006}(p. 279). In other words, when $\lambda$ is estimated by REML, the REML error variance estimate agrees with the "smoothing-theoretic" variance estimate.

\subsection{\textbf{Derivative of GCV}}
The GCV criterion is given by
\begin{align*}
GCV(\lambda)=\frac{\parallel \by-\hat{\by}\parallel ^2}{[1-tr(\bH_\lambda)/n]^2}=\frac{\by^T (\bI-\bH_\lambda)^2\by}{[tr(\bI-\bH_\lambda)]^2}=\frac{\by^T \bP_\lambda^2\by}{[tr(\bP_\lambda)]^2}
\end{align*}
with the last equality following from (1.7). This criterion, originally proposed by \citep{craven_smoothing_1979}, is an approximation to $\frac{1}{n}\sum_{i=1}^n \frac{(y_i-\hat{y}_i)^2}{(1-h_{\lambda [ii]})^2}$, where $h_{\lambda [11]},...,h_{\lambda [nn]}$ are the diagonal elements of $\bH_\lambda$. The latter expression can be shown (at least in some smoothing problems) to be equal to the leave-one-out cross-validation criterion, but lacks an invariance-under-reparametrization property that is gained by instead using GCV \citep{wahba_spline_1990}(pp. 52-53). Using (1.14), we can obtain
\begin{align}
\frac{\partial GCV(\lambda)}{\partial \lambda}=\frac{2}{\lambda [tr(\bP_\lambda)]^3}\Big[tr(\bP_\lambda^2)\by^T P_\lambda^2\by-tr(\bP_\lambda)\by^T P_\lambda^3\by \Big]
\end{align}
Thus at the GCV-minimizing $\lambda$ we have
\begin{align*}
\frac{\by^T \bP_\lambda^3\by}{tr(\bP_\lambda^2)}=\frac{\by^T \bP_\lambda^2\by}{tr(\bP_\lambda)}\quad or\quad \frac{\by^T \bP_\lambda^3\by}{\by^T \bP_\lambda^2\by}=\frac{tr(\bP_\lambda^2)}{tr(\bP_\lambda)}
\end{align*}

\subsection{\textbf{Derivative of AIC}}
\begin{comment}
According to (2.1.5), Akaike's Information Criterion (AIC) chooses $\lambda$ by minimizing,
\begin{align*}
AIC=2(p+2)+n log[\frac{1}{n}(\by-\bmu)^T(\bI_n-\bA_\lambda)^2(\by-\bmu)]+n+n log(2\pi)
\end{align*}
For given $\lambda$, the value of $\mu$ minimizing this expression is the generalized least square fit $\hat{\mu}_\lambda=[\mathbf{1}^T(\bI_n-\bA_\lambda)^2\mathbf{1}]^{-1}\mathbf{1}^T(\bI_n-\bA_\lambda)^2\by=(\mathbf{1}^T\bV_\lambda^{-2}\mathbf{1})^{-1}\mathbf{1}^T\bV_\lambda^{-2}\by$.\\

Ignoring an additive constant leads to the objective function
\end{comment}

The AIC criterion is given by
\begin{align*}
AIC(\lambda)=log (\parallel \by-\hat{\by}\parallel ^2)+\frac{2}{n}[tr(\bH_\lambda)+1]=log(\by^T \bP_\lambda^2\by)+\frac{2}{n}tr(\bI-\bP_\lambda)+\frac{2}{n}
\end{align*}
Using (1.14), we can obtain
\begin{align}
\frac{\partial AIC(\lambda)}{\partial \lambda}=\frac{2}{\lambda}\Big[1-\frac{\by^T \bP_\lambda^3\by}{\by^T \bP_\lambda^2\by}-\frac{1}{n}tr(\bP_\lambda-\bP_\lambda^2)\Big]
\end{align}
Thus at the AIC-minimizing $\lambda$ we have
\begin{align*}
\frac{\by^T \bP_\lambda^3\by}{\by^T \bP_\lambda^2\by}=\frac{tr(\bI-\bP_\lambda+\bP_\lambda^2)}{n}
\end{align*}

\clearpage
\bibliography{reference2}


%----------------------------------------------------------------------------------------
\end{document}
