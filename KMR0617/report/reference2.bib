
@article{maity_powerful_2011,
	title = {Powerful tests for detecting a gene effect in the presence of possible gene-gene interactions using garrote kernel machines},
	volume = {67},
	issn = {1541-0420},
	doi = {10.1111/j.1541-0420.2011.01598.x},
	abstract = {We propose in this article a powerful testing procedure for detecting a gene effect on a continuous outcome in the presence of possible gene-gene interactions (epistasis) in a gene set, e.g., a genetic pathway or network. Traditional tests for this purpose require a large number of degrees of freedom by testing the main effect and all the corresponding interactions under a parametric assumption, and hence suffer from low power. In this article, we propose a powerful kernel machine based test. Specifically, our test is based on a garrote kernel method and is constructed as a score test. Here, the term garrote refers to an extra nonnegative parameter that is multiplied to the covariate of interest so that our score test can be formulated in terms of this nonnegative parameter. A key feature of the proposed test is that it is flexible and developed for both parametric and nonparametric models within a unified framework, and is more powerful than the standard test by accounting for the correlation among genes and hence often uses a much smaller degrees of freedom. We investigate the theoretical properties of the proposed test. We evaluate its finite sample performance using simulation studies, and apply the method to the Michigan prostate cancer gene expression data.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Maity, Arnab and Lin, Xihong},
	month = dec,
	year = {2011},
	pmid = {21504419},
	pmcid = {PMC3142308},
	keywords = {Algorithms, Artificial Intelligence, Biomarkers, Genetic Predisposition to Disease, Humans, Male, Michigan, Prostatic Neoplasms, Protein Interaction Mapping, Tumor},
	pages = {1271--1284}
}

@article{reiss_smoothing_2009,
	title = {Smoothing {Parameter} {Selection} for a {Class} of {Semiparametric} {Linear} {Models}},
	volume = {71},
	abstract = {Berkeley Electronic Press Selected Works, Spline-based approaches to nonparametric and semiparametric regression, as well as to regression of scalar outcomes on functional predictors, entail choosing a parameter controlling the extent to which roughness of the fitted function is penalized. In this paper we demonstrate that the equations determining two popular methods for smoothing parameter selection, generalized cross-validation and restricted maximum likelihood, share a similar form that allows us to prove several results common to both, and to derive a condition under which they yield identical values. These ideas are illustrated by application of functional principal component regression, a method for regressing scalars on functions, to two chemometric data sets.},
	language = {en},
	number = {2},
	urldate = {2018-06-02},
	journal = {JRSS-B},
	author = {Reiss, Philip T. and Ogden, R. Todd},
	year = {2009},
	file = {Full Text PDF:/Users/dorabeedeng/Zotero/storage/WNMWF2CS/Reiss and Ogden - 2009 - Smoothing Parameter Selection for a Class of Semip.pdf:application/pdf;Snapshot:/Users/dorabeedeng/Zotero/storage/M2GS4UIC/1.html:text/html}
}

@article{lindstrom_newtonraphson_1988,
	title = {Newton—{Raphson} and {EM} {Algorithms} for {Linear} {Mixed}-{Effects} {Models} for {Repeated}-{Measures} {Data}},
	volume = {83},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.1988.10478693},
	doi = {10.1080/01621459.1988.10478693},
	abstract = {We develop an efficient and effective implementation of the Newton—Raphson (NR) algorithm for estimating the parameters in mixed-effects models for repeated-measures data. We formulate the derivatives for both maximum likelihood and restricted maximum likelihood estimation and propose improvements to the algorithm discussed by Jennrich and Schlüchter (1986) to speed convergence and ensure a positive-definite covariance matrix for the random effects at each iteration. We use matrix decompositions to develop efficient and computationally stable implementations of both the NR algorithm and an EM algorithm (Laird and Ware 1982) for this model. We compare the two methods (EM vs. NR) in terms of computational order and performance on two sample data sets and conclude that in most situations a well-implemented NR algorithm is preferable to the EM algorithm or EM algorithm with Aitken's acceleration. The term repeated measures refers to experimental designs where there are several individuals and several measurements taken on each individual. In the mixed-effects model each individual's vector of responses is modeled as a parametric function, where some of the parameters or “effects” are random variables with a multivariate normal distribution. This model has been successful because it can handle unbalanced data (different designs for different individuals), missing data (observations on all individuals are taken at the same design points, but some individuals have missing data), and jointly dependent random effects. The price for this flexibility is that the parameter estimates may be difficult to compute. We propose some new methods for implementing the EM and NR algorithms and draw conclusions about their performance. We also discuss extensions of the mixed-effects model to incoporate nonindependent conditional error structure and nested-type designs.},
	number = {404},
	urldate = {2018-06-02},
	journal = {Journal of the American Statistical Association},
	author = {Lindstrom, Mary J. and Bates, Douglas M.},
	month = dec,
	year = {1988},
	keywords = {Growth curve, Longitudinal data, Random effects},
	pages = {1014--1022},
	file = {Snapshot:/Users/dorabeedeng/Zotero/storage/3ZRHRR6U/01621459.1988.html:text/html}
}

@book{wahba_spline_1990,
	series = {{CBMS}-{NSF} {Regional} {Conference} {Series} in {Applied} {Mathematics}},
	title = {Spline {Models} for {Observational} {Data}},
	isbn = {978-0-89871-244-5},
	url = {https://epubs.siam.org/doi/book/10.1137/1.9781611970128},
	abstract = {This monograph is based on a series of 10 lectures at Ohio State University at Columbus, March 23–27, 1987, sponsored by the Conference Board of the Mathematical Sciences and the National Science Foundation. The selection of topics is quite personal and, together with the talks of the other speakers, the lectures represent a story, as I saw it in March 1987, of many of the interesting things that statisticians can do with splines. I told the audience that the priority order for topic selection was, first, obscure work of my own and collaborators, second, other work by myself and students, with important work by other speakers deliberately omitted in the hope that they would mention it themselves. This monograph will more or less follow that outline, so that it is very much slanted toward work I had some hand in, although I will try to mention at least by reference important work by the other speakers and some of the attendees. The other speakers were (in alphabetical order), Dennis Cox, Randy Eubank, Ker-Chau Li, Douglas Nychka, David Scott, Bernard Silverman, Paul Speckman, and James Wendelberger. The work of Finbarr O'Sullivan, who was unable to attend, in extending the developing theory to the non-Gaussian and nonlinear case will also play a central role, as will the work of Florencio Utreras.},
	urldate = {2018-06-02},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Wahba, G.},
	month = jan,
	year = {1990},
	doi = {10.1137/1.9781611970128},
	file = {Snapshot:/Users/dorabeedeng/Zotero/storage/RAVRYN57/1.html:text/html}
}

@book{pawitan_all_2001,
	address = {Oxford, New York},
	title = {In {All} {Likelihood}: {Statistical} {Modelling} and {Inference} {Using} {Likelihood}},
	isbn = {978-0-19-850765-9},
	shorttitle = {In {All} {Likelihood}},
	abstract = {This text concentrates on what can be achieved using the likelihood/Fisherian methods of taking into account uncertainty when studying a statistical problem. It takes the concept of the likelihood as the best method for unifying the demands of statistical modeling and theory of inference. Every likelihood concept is illustrated with realistic examples ranging from a simple comparison of two accident rates to complex studies that require generalized linear or semiparametric modeling. The emphasis is on likelihood not as just a device used to produce an estimate, but as an important tool for modeling.},
	publisher = {Oxford University Press},
	author = {Pawitan},
	month = aug,
	year = {2001},
	file = {Snapshot:/Users/dorabeedeng/Zotero/storage/8V22S3UL/in-all-likelihood-9780198507659.html:text/html}
}

@misc{lee_generalized_2006,
	title = {Generalized {Linear} {Models} with {Random} {Effects}: {Unified} {Analysis} via {H}-likelihood},
	shorttitle = {Generalized {Linear} {Models} with {Random} {Effects}},
	abstract = {Since their introduction in 1972, generalized linear models (GLMs) have proven useful in the generalization of classical normal models. Presenting methods for fitting GLMs with random effects to data, Generalized Linear Models with Random Effects: Unified Analysis via H-likelihood explores a wide ra},
	language = {en},
	urldate = {2018-06-02},
	journal = {CRC Press},
	author = {Lee and Nelder and Pawitan},
	month = jul,
	year = {2006},
	file = {Snapshot:/Users/dorabeedeng/Zotero/storage/2L6DDHL8/9781420011340.html:text/html}
}

@misc{craven_smoothing_1979,
	title = {Smoothing noisy data with spline functions {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/BF01404567},
	urldate = {2018-06-02},
	author = {Craven, P. and Wahba, G.},
	year = {1979},
	file = {Smoothing noisy data with spline functions | SpringerLink:/Users/dorabeedeng/Zotero/storage/ZBTXRM4J/BF01404567.html:text/html}
}