---
title: "Using the CVEK R package"
author: "Wenying Deng"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using the CVEK R package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Short Description

Using a library of base kernels, **CVEK** learns a proper 
generating function from data by directly minimizing the ensemble 
model’s error, and tests whether the data is generated by the RKHS 
under the null hypothesis.

Download the package from CRAN or [GitHub](https://github.com/IrisTeng/CVEK) and then install and load it.

```{r, message = FALSE}
devtools::install_github("IrisTeng/CVEK")
library(CVEK)
```

## Test Functions
### Model Definition

*defineModel* function has four returns with four parameters.
```{r}
defineModel
```

Note that:

* All four parameters are mandatory for management.
* Users can generate *Kern_par* with the function *kernelGenerate*.
* If users want to test data with given and known interaction strength, they
  can generate data with the function *dataGenerate*.

For example, we have a dataset whose $size=100$, $label\_names=list(X1 = c("x1", "x2"), X2 = c("x3", "x4"))$, $int\_effect=0.3$, $method="rbf"$ with $l=1$ and $eps=0.01$:

```{r}
label_names <- list(X1 = c("x1", "x2"), X2 = c("x3", "x4"))
data <- dataGenerate(size = 100, label_names, method = "rbf", 
                     int_effect = 0, l = 1, eps = 0.01)
```

```{r, echo=FALSE, results='asis'}
knitr::kable(head(data, 10))
```

we want our kernel library contains three kernels: $"rbf"$ with $l=0.5$, $"polynomial"$ with $p=2$, and $"matern"$ with $l=1.5, p=3$:

```{r}
Kern_par <- data.frame(method = c("rbf", "polynomial", "matern"), 
                       Sigma = rep(0, 3), l = c(.5, 1, 1.5), p = 1:3)
Kern_par$method <- as.character(Kern_par$method)
```

and the null model is $Y \sim X1 + X2$:
```{r}
formula <- Y ~ X1 + X2
```

With all these parameters specified, we can define our model:

```{r}
fit <- defineModel(formula, label_names, data, Kern_par)
# fit$Y
# fit$X1
# fit$X2
# fit$Kernlist
```


### Estimation

After defining the model, we can apply *estimation* function to conduct gaussian 
process regression based on the estimated ensemble kernel matrix.

*estimation* function has five returns with eight parameters.
```{r}
estimation
```

Note that:

* The first four parameters are mandatory for management.
* For the last four parameters, users can substitute alternatives for the 
  default ones.
  
Continuing with our example, we want to use $"erm"$ to ensemble our base kernels and $"loocv"$ to select tuning parameter whose range is $lambda=exp(seq(-5, 5))$:

```{r}
mode <- "loocv"
strategy <- "erm"
lambda <- exp(seq(-5, 5))
```

Then we can find the solution with the help of *estimation*:

```{r}
sol <- estimation(fit$Y, fit$X1, fit$X2, fit$Kernlist, mode, strategy, lambda)
# sol$lam
# sol$intercept
# sol$alpha
# sol$K
# sol$u_hat
```


### Testing

Finally, here comes the testing procedure.

*testing* function has one return with several parameters.
```{r}
testing
```

Note that $formula\_int$ is the alternative model with interaction.

Now, we want to conduct score test with $test="boot"$ and $B=100$ since the sample size is small ($size=100$).

```{r}
formula_int <- Y ~ X1 * X2
test <- "boot"
B <- 100
pvalue <- testing(formula_int, label_names, fit$Y, fit$X1, fit$X2, fit$Kernlist, 
                  mode, strategy, beta = 1, test, lambda, B)
pvalue
```

Additionally, we simulated a couple of results based on the following parameters:

* kernel library: 3 polynomial kernels ($p = 1, 2, 3$) and 3 RBF kernels ($l = 0.6,   1, 2$).
* size=100, test="boot".

```{r, fig.width=14, fig.height=9}
knitr::include_graphics("B4.pdf", auto_pdf = TRUE)
```

## References

1.  Jeremiah Zhe Liu and Brent Coull. Robust Hypothesis Test for Nonlinear Effect       with Gaus- sian Processes. October 2017.
1.  Xiang Zhan, Anna Plantinga, Ni Zhao, and Michael C. Wu. A fast small-sample         kernel inde- pendence test for microbiome community-level association analysis.     December 2017.
1.  Arnak S. Dalalyan and Alexandre B. Tsybakov. Aggregation by Exponential             Weighting and Sharp Oracle Inequalities. In Learning Theory, Lecture Notes in       Computer Science, pages 97– 111. Springer, Berlin, Heidelberg, June 2007.
1.  Arnab Maity and Xihong Lin. Powerful tests for detecting a gene effect in the       presence of possible gene-gene interactions using garrote kernel machines.          December 2011.
1.  The MIT Press. Gaussian Processes for Machine Learning, 2006.
1.  Xihong Lin. Variance component testing in generalised linear models with random     effects. June 1997.
1.  Philip S. Boonstra, Bhramar Mukherjee, and Jeremy M. G. Taylor. A Small-Sample      Choice of the Tuning Parameter in Ridge Regression. July 2015.
1.  Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of              Statistical Learning: Data Mining, Inference, and Prediction, Second Edition.       Springer Series in Statistics. Springer- Verlag, New York, 2 edition, 2009.
1.  Hirotogu Akaike. Information Theory and an Extension of the Maximum Likelihood      Princi- ple. In Selected Papers of Hirotugu Akaike, Springer Series in              Statistics, pages 199–213. Springer, New York, NY, 1998.
1.  Clifford M. Hurvich and Chih-Ling Tsai. Regression and time series model            selection in small samples. June 1989.
1.  Hurvich Clifford M., Simonoff Jeffrey S., and Tsai Chih-Ling. Smoothing             parameter selection in nonparametric regression using an improved Akaike            information criterion. January 2002.
