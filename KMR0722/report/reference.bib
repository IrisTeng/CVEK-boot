
@article{liu_robust_2017,
	title = {Robust {Hypothesis} {Test} for {Nonlinear} {Effect} with {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/1710.01406},
	abstract = {This work constructs a hypothesis test for detecting whether an data-generating function \$h: Rˆp {\textbackslash}textbackslashrightarrow R\$ belongs to a specific reproducing kernel Hilbert space \${\textbackslash}textbackslashmathcal\{H\}\_0\$ , where the structure of \${\textbackslash}textbackslashmathcal\{H\}\_0\$ is only partially known. Utilizing the theory of reproducing kernels, we reduce this hypothesis to a simple one-sided score test for a scalar parameter, develop a testing procedure that is robust against the mis-specification of kernel functions, and also propose an ensemble-based estimator for the null model to guarantee test performance in small samples. To demonstrate the utility of the proposed method, we apply our test to the problem of detecting nonlinear interaction between groups of continuous features. We evaluate the finite-sample performance of our test under different data-generating functions and estimation strategies for the null model. Our results reveal interesting connections between notions in machine learning (model underfit/overfit) and those in statistical inference (i.e. Type I error/power of hypothesis test), and also highlight unexpected consequences of common model estimating strategies (e.g. estimating kernel hyperparameters using maximum likelihood estimation) on model inference.},
	urldate = {2018-05-06},
	journal = {arXiv:1710.01406 [stat]},
	author = {Liu, Jeremiah Zhe and Coull, Brent},
	month = oct,
	year = {2017},
	keywords = {Jeremiah, Liu, Statistics - Machine Learning},
	annote = {arXiv: 1710.01406}
}

@article{golub_generalized_1979,
	title = {Generalized {Cross}-{Validation} as a {Method} for {Choosing} a {Good} {Ridge} {Parameter}},
	volume = {21},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00401706.1979.10489751},
	doi = {10.1080/00401706.1979.10489751},
	abstract = {Consider the ridge estimate (λ) for β in the model unknown, (λ) = (X T X + nλI)−1 X T y. We study the method of generalized cross-validation (GCV) for choosing a good value for λ from the data. The estimate is the minimizer of V(λ) given by where A(λ) = X(X T X + nλI)−1 X T . This estimate is a rotation-invariant version of Allen's PRESS, or ordinary cross-validation. This estimate behaves like a risk improvement estimator, but does not require an estimate of σ2, so can be used when n − p is small, or even if p ≥ 2 n in certain cases. The GCV method can also be used in subset selection and singular value truncation methods for regression, and even to choose from among mixtures of these methods.},
	number = {2},
	urldate = {2018-05-10},
	journal = {Technometrics},
	author = {Golub, Gene H. and Heath, Michael and Wahba, Grace},
	month = may,
	year = {1979},
	keywords = {Cross-validation, Ridge parameter, Ridge regression},
	pages = {215--223}
}

@article{hurvich_clifford_m._smoothing_2002,
	title = {Smoothing parameter selection in nonparametric regression using an improved {Akaike} information criterion},
	volume = {60},
	issn = {1369-7412},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00125},
	doi = {10.1111/1467-9868.00125},
	abstract = {Many different methods have been proposed to construct nonparametric estimates of a smooth regression function, including local polynomial, (convolution) kernel and smoothing spline estimators. Each of these estimators uses a smoothing parameter to control the amount of smoothing performed on a given data set. In this paper an improved version of a criterion based on the Akaike information criterion (AIC), termed AICC, is derived and examined as a way to choose the smoothing parameter. Unlike plug?in methods, AICC can be used to choose smoothing parameters for any linear smoother, including local quadratic and smoothing spline estimators. The use of AICC avoids the large variability and tendency to undersmooth (compared with the actual minimizer of average squared error) seen when other ?classical? approaches (such as generalized cross?validation (GCV) or the AIC) are used to choose the smoothing parameter. Monte Carlo simulations demonstrate that the AICC?based smoothing parameter is competitive with a plug?in method (assuming that one exists) when the plug?in method works well but also performs well when the plug?in approach fails or is unavailable.},
	number = {2},
	urldate = {2018-05-10},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {{Hurvich Clifford M.} and {Simonoff Jeffrey S.} and {Tsai Chih‐Ling}},
	month = jan,
	year = {2002},
	keywords = {Convolution kernel regression estimator, Local polynomial regression estimator, Plug‐in method, Smoothing spline regression estimator},
	pages = {271--293}
}

@article{hurvich_regression_1989,
	title = {Regression and time series model selection in small samples},
	volume = {76},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/76/2/297/265326},
	doi = {10.1093/biomet/76.2.297},
	abstract = {Abstract. A bias correction to the Akaike information criterion, AIC, is derived for regression and autoregressive time series models. The correction is of par},
	language = {en},
	number = {2},
	urldate = {2018-05-10},
	journal = {Biometrika},
	author = {Hurvich, Clifford M. and Tsai, Chih-Ling},
	month = jun,
	year = {1989},
	pages = {297--307}
}

@article{wecker_signal_1983,
	title = {The {Signal} {Extraction} {Approach} to {Nonlinear} {Regression} and {Spline} {Smoothing}},
	volume = {78},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1983.10477935},
	doi = {10.1080/01621459.1983.10477935},
	abstract = {This article shows how to fit a smooth curve (polynomial spline) to pairs of data values (yi, xi ). Prior specification of a parametric functional form for the curve is not required. The resulting curve can be used to describe the pattern of the data, and to predict unknown values of y given x. Both point and interval estimates are produced. The method is easy to use, and the computational requirements are modest, even for large sample sizes. Our method is based on maximum likelihood estimation of a signal-in-noise model of the data. We use the Kalman filter to evaluate the likelihood function and achieve significant computational advantages over previous approaches to this problem.},
	number = {381},
	urldate = {2018-05-10},
	journal = {Journal of the American Statistical Association},
	author = {Wecker, William E. and Ansley, Craig F.},
	month = mar,
	year = {1983},
	pages = {81--89}
}

@article{wahba_comparison_1985,
	title = {A {Comparison} of {GCV} and {GML} for {Choosing} the {Smoothing} {Parameter} in the {Generalized} {Spline} {Smoothing} {Problem}},
	volume = {13},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176349743},
	doi = {10.1214/aos/1176349743},
	abstract = {The partially improper prior behind the smoothing spline model is used to obtain a generalization of the maximum likelihood (GML) estimate for the smoothing parameter. Then this estimate is compared with the generalized cross validation (GCV) estimate both analytically and by Monte Carlo methods. The comparison is based on a predictive mean square error criteria. It is shown that if the true, unknown function being estimated is smooth in a sense to be defined then the GML estimate undersmooths relative to the GCV estimate and the predictive mean square error using the GML estimate goes to zero at a slower rate than the mean square error using the GCV estimate. If the true function is "rough" then the GCV and GML estimates have asymptotically similar behavior. A Monte Carlo experiment was designed to see if the asymptotic results in the smooth case were evident in small sample sizes. Mixed results were obtained for n=32n=32n = 32, GCV was somewhat better than GML for n=64n=64n = 64, and GCV was decidedly superior for n=128n=128n = 128. In the n=32n=32n = 32 case GCV was better for smaller σ2σ2{\textbackslash}textbackslashsigmaˆ2 and the comparison close for larger σ2σ2{\textbackslash}textbackslashsigmaˆ2. The theoretical results are shown to extend to the generalized spline smoothing model, which includes the estimate of functions given noisy values of various integrals of them.},
	language = {EN},
	number = {4},
	urldate = {2018-05-10},
	journal = {The Annals of Statistics},
	author = {Wahba, Grace},
	month = dec,
	year = {1985},
	mrnumber = {MR811498},
	zmnumber = {0596.65004},
	keywords = {cross validation, integral equations, maximum likelihood, Spline smoothing},
	pages = {1378--1402}
}

@book{hastie_elements_2009,
	address = {New York},
	edition = {2},
	series = {Springer {Series} in {Statistics}},
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}, {Second} {Edition}},
	isbn = {978-0-387-84857-0},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	url = {//www.springer.com/fr/book/9780387848570},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting—the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression and path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for “wide” data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
	language = {en},
	urldate = {2018-05-10},
	publisher = {Springer-Verlag},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	year = {2009}
}

@article{boonstra_small-sample_2015,
	title = {A {Small}-{Sample} {Choice} of the {Tuning} {Parameter} in {Ridge} {Regression}},
	volume = {25},
	issn = {1017-0405},
	doi = {10.5705/ss.2013.284},
	abstract = {We propose new approaches for choosing the shrinkage parameter in ridge regression, a penalized likelihood method for regularizing linear regression coefficients, when the number of observations is small relative to the number of parameters. Existing methods may lead to extreme choices of this parameter, which will either not shrink the coefficients enough or shrink them by too much. Within this "small-n, large-p" context, we suggest a correction to the common generalized cross-validation (GCV) method that preserves the asymptotic optimality of the original GCV. We also introduce the notion of a "hyperpenalty", which shrinks the shrinkage parameter itself, and make a specific recommendation regarding the choice of hyperpenalty that empirically works well in a broad range of scenarios. A simple algorithm jointly estimates the shrinkage parameter and regression coefficients in the hyperpenalized likelihood. In a comprehensive simulation study of small-sample scenarios, our proposed approaches offer superior prediction over nine other existing methods.},
	language = {eng},
	number = {3},
	journal = {Statistica Sinica},
	author = {Boonstra, Philip S. and Mukherjee, Bhramar and Taylor, Jeremy M. G.},
	month = jul,
	year = {2015},
	pmid = {26985140},
	pmcid = {PMC4790465},
	keywords = {Cross-validation, Akaike’s information criterion, Generalized cross-validation, Hyperpenalty, Marginal likelihood, Penalized likelihood},
	pages = {1185--1206}
}

@article{wang_review_2018,
	title = {A {Review} of {Application} of {Affordance} {Theory} in {Information} {Systems}},
	volume = {11},
	issn = {1940-9893, 1940-9907},
	url = {http://www.scirp.org/journal/doi.aspx?DOI=10.4236/jssm.2018.111006},
	doi = {10.4236/jssm.2018.111006},
	number = {01},
	urldate = {2018-05-10},
	journal = {Journal of Service Science and Management},
	author = {Wang, Huifen and Wang, Jialu and Tang, Qiuhong},
	year = {2018},
	pages = {56--70}
}

@article{zhan_fast_2017,
	title = {A fast small-sample kernel independence test for microbiome community-level association analysis},
	volume = {73},
	issn = {0006-341X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5592124/},
	doi = {10.1111/biom.12684},
	abstract = {To fully understand the role of microbiome in human health and diseases, researchers are increasingly interested in assessing the relationship between microbiome composition and host genomic data. The dimensionality of the data as well as complex relationships between microbiota and host genomics pose considerable challenges for analysis. In this paper, we apply a kernel RV coefficient (KRV) test to evaluate the overall association between host gene expression and microbiome composition. The KRV statistic can capture non-linear correlations and complex relationships among the individual data types and between gene expression and microbiome composition through measuring general dependency. Testing proceeds via a similar route as existing tests of the generalized RV coefficients and allows for rapid p-value calculation. Strategies to allow adjustment for confounding effects, which is crucial for avoiding misleading results, and to alleviate the problem of selecting the most favorable kernel are considered. Simulation studies show that KRV is useful in testing statistical independence with finite samples given the kernels are appropriately chosen, and can powerfully identify existing associations between microbiome composition and host genomic data while protecting type I error. We apply the KRV to a microbiome study examining the relationship between host transcriptome and microbiome composition within the context of inflammatory bowel disease and are able to derive new biological insights and provide formal inference on prior qualitative observations.},
	number = {4},
	urldate = {2018-05-10},
	journal = {Biometrics},
	author = {Zhan, Xiang and Plantinga, Anna and Zhao, Ni and Wu, Michael C.},
	month = dec,
	year = {2017},
	pmid = {28295177},
	pmcid = {PMC5592124},
	pages = {1453--1463}
}

@article{lin_variance_1997,
	title = {Variance component testing in generalised linear models with random effects},
	volume = {84},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/84/2/309/233889},
	doi = {10.1093/biomet/84.2.309},
	abstract = {Abstract. There is considerable interest in testing for overdispersion, correlation and heterogeneity across groups in biomedical studies. In this paper, we ca},
	language = {en},
	number = {2},
	urldate = {2018-05-10},
	journal = {Biometrika},
	author = {Lin, Xihong},
	month = jun,
	year = {1997},
	pages = {309--326}
}

@article{maity_powerful_2011,
	title = {Powerful tests for detecting a gene effect in the presence of possible gene-gene interactions using garrote kernel machines},
	volume = {67},
	issn = {1541-0420},
	doi = {10.1111/j.1541-0420.2011.01598.x},
	abstract = {We propose in this article a powerful testing procedure for detecting a gene effect on a continuous outcome in the presence of possible gene-gene interactions (epistasis) in a gene set, e.g., a genetic pathway or network. Traditional tests for this purpose require a large number of degrees of freedom by testing the main effect and all the corresponding interactions under a parametric assumption, and hence suffer from low power. In this article, we propose a powerful kernel machine based test. Specifically, our test is based on a garrote kernel method and is constructed as a score test. Here, the term garrote refers to an extra nonnegative parameter that is multiplied to the covariate of interest so that our score test can be formulated in terms of this nonnegative parameter. A key feature of the proposed test is that it is flexible and developed for both parametric and nonparametric models within a unified framework, and is more powerful than the standard test by accounting for the correlation among genes and hence often uses a much smaller degrees of freedom. We investigate the theoretical properties of the proposed test. We evaluate its finite sample performance using simulation studies, and apply the method to the Michigan prostate cancer gene expression data.},
	language = {eng},
	number = {4},
	journal = {Biometrics},
	author = {Maity, Arnab and Lin, Xihong},
	month = dec,
	year = {2011},
	pmid = {21504419},
	pmcid = {PMC3142308},
	keywords = {Algorithms, Artificial Intelligence, Biomarkers, Genetic Predisposition to Disease, Humans, Male, Michigan, Prostatic Neoplasms, Protein Interaction Mapping, Tumor},
	pages = {1271--1284}
}

@article{harville_maximum_1977,
	title = {Maximum {Likelihood} {Approaches} to {Variance} {Component} {Estimation} and to {Related} {Problems}},
	volume = {72},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2286796},
	doi = {10.2307/2286796},
	abstract = {Recent developments promise to increase greatly the popularity of maximum likelihood (ML) as a technique for estimating variance components. Patterson and Thompson (1971) proposed a restricted maximum likelihood (REML) approach which takes into account the loss in degrees of freedom resulting from estimating fixed effects. Miller (1973) developed a satisfactory asymptotic theory for ML estimators of variance components. There are many iterative algorithms that can be considered for computing the ML or REML estimates. The computations on each iteration of these algorithms are those associated with computing estimates of fixed and random effects for given values of the variance components.},
	number = {358},
	urldate = {2018-05-10},
	journal = {Journal of the American Statistical Association},
	author = {Harville, David A.},
	year = {1977},
	pages = {320--338}
}

@article{zhang_hypothesis_2003,
	title = {Hypothesis testing in semiparametric additive mixed models},
	volume = {4},
	issn = {1465-4644},
	doi = {10.1093/biostatistics/4.1.57},
	abstract = {We consider testing whether the nonparametric function in a semiparametric additive mixed model is a simple fixed degree polynomial, for example, a simple linear function. This test provides a goodness-of-fit test for checking parametric models against nonparametric models. It is based on the mixed-model representation of the smoothing spline estimator of the nonparametric function and the variance component score test by treating the inverse of the smoothing parameter as an extra variance component. We also consider testing the equivalence of two nonparametric functions in semiparametric additive mixed models for two groups, such as treatment and placebo groups. The proposed tests are applied to data from an epidemiological study and a clinical trial and their performance is evaluated through simulations.},
	language = {eng},
	number = {1},
	journal = {Biostatistics (Oxford, England)},
	author = {Zhang, Daowen and Lin, Xihong},
	month = jan,
	year = {2003},
	pmid = {12925330},
	keywords = {Humans, Male, Anticonvulsants, Child, Computer Simulation, Data Interpretation, Epilepsy, Female, gamma-Aminobutyric Acid, Indonesia, Likelihood Functions, Longitudinal Studies, Models, Nonparametric, Placebos, Randomized Controlled Trials as Topic, Respiratory Tract Infections, Statistical, Statistics, Xerophthalmia},
	pages = {57--74}
}

@article{evgeniou_leave_2004,
	title = {Leave {One} {Out} {Error}, {Stability}, and {Generalization} of {Voting} {Combinations} of {Classifiers}},
	volume = {55},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/B:MACH.0000019805.88351.60},
	doi = {10.1023/B:MACH.0000019805.88351.60},
	abstract = {We study the leave-one-out and generalization errors of voting combinations of learning machines. A special case considered is a variant of bagging. We analyze in detail combinations of kernel machines, such as support vector machines, and present theoretical estimates of their leave-one-out error. We also derive novel bounds on the stability of combinations of any classifiers. These bounds can be used to formally show that, for example, bagging increases the stability of unstable learning machines. We report experiments supporting the theoretical findings.},
	language = {en},
	number = {1},
	urldate = {2018-05-10},
	journal = {Machine Learning},
	author = {Evgeniou, Theodoros and Pontil, Massimiliano and Elisseeff, André},
	month = apr,
	year = {2004},
	pages = {71--97}
}

@article{cortes_ensembles_2012,
	title = {Ensembles of {Kernel} {Predictors}},
	url = {http://arxiv.org/abs/1202.3712},
	abstract = {This paper examines the problem of learning with a finite and possibly large set of p base kernels. It presents a theoretical and empirical analysis of an approach addressing this problem based on ensembles of kernel predictors. This includes novel theoretical guarantees based on the Rademacher complexity of the corresponding hypothesis sets, the introduction and analysis of a learning algorithm based on these hypothesis sets, and a series of experiments using ensembles of kernel predictors with several data sets. Both convex combinations of kernel-based hypotheses and more general Lq-regularized nonnegative combinations are analyzed. These theoretical, algorithmic, and empirical results are compared with those achieved by using learning kernel techniques, which can be viewed as another approach for solving the same problem.},
	urldate = {2018-05-10},
	journal = {arXiv:1202.3712 [cs, stat]},
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	month = feb,
	year = {2012},
	keywords = {Statistics - Machine Learning, Computer Science - Learning},
	annote = {arXiv: 1202.3712}
}

@article{buzkova_permutation_2011,
	title = {Permutation and parametric bootstrap tests for gene-gene and gene-environment interactions},
	volume = {75},
	issn = {1469-1809},
	doi = {10.1111/j.1469-1809.2010.00572.x},
	abstract = {Permutation tests are widely used in genomic research as a straightforward way to obtain reliable statistical inference without making strong distributional assumptions. However, in this paper we show that in genetic association studies it is not typically possible to construct exact permutation tests of gene-gene or gene-environment interaction hypotheses. We describe an alternative to the permutation approach in testing for interaction, a parametric bootstrap approach. Using simulations, we compare the finite-sample properties of a few often-used permutation tests and the parametric bootstrap. We consider interactions of an exposure with single and multiple polymorphisms. Finally, we address when permutation tests of interaction will be approximately valid in large samples for specific test statistics.},
	language = {eng},
	number = {1},
	journal = {Annals of Human Genetics},
	author = {Bůžková, Petra and Lumley, Thomas and Rice, Kenneth},
	month = jan,
	year = {2011},
	pmid = {20384625},
	pmcid = {PMC2904826},
	keywords = {Humans, Computer Simulation, Models, Statistical, Animals, Environment, Epistasis, Genetic, Genetic Association Studies, Polymorphism},
	pages = {36--45}
}

@incollection{akaike_information_1998,
	series = {Springer {Series} in {Statistics}},
	title = {Information {Theory} and an {Extension} of the {Maximum} {Likelihood} {Principle}},
	isbn = {978-1-4612-7248-9 978-1-4612-1694-0},
	abstract = {In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting.},
	language = {en},
	urldate = {2018-05-11},
	booktitle = {Selected {Papers} of {Hirotugu} {Akaike}},
	publisher = {Springer, New York, NY},
	author = {Akaike, Hirotogu},
	year = {1998},
	pages = {199--213}
}

@article{anderson_comparison_1998,
	title = {Comparison of {Akaike} information criterion and consistent {Akaike} information criterion for model selection and statistical inference from capture-recapture studies},
	volume = {25},
	issn = {0266-4763},
	url = {https://www.tandfonline.com/doi/abs/10.1080/02664769823250},
	doi = {10.1080/02664769823250},
	abstract = {We compare properties of parameter estimators under Akaike information criterion (AIC) and 'consistent' AIC (CAIC) model selection in a nested sequence of open population capture-recapture models. These models consist of product multinomials, where the cell probabilities are parameterized in terms of survival and capture ( p ) i i probabilities for each time interval i . The sequence of models is derived from 'treatment' effects that might be (1) absent, model H ; (2) only acute, model H ; or (3) acute and 0 2 p chronic, lasting several time intervals, model H . Using a 35 factorial design, 1000 3 repetitions were simulated for each of 243 cases. The true number of parameters ranged from 7 to 42, and the sample size ranged from approximately 470 to 55 000 per case. We focus on the quality of the inference about the model parameters and model structure that results from the two selection criteria. We use achieved confidence interval coverage as an integrating metric to judge what constitutes a 'properly parsimonious' model, and contrast the performance of these two model selection criteria for a wide range of models, sample sizes, parameter values and study interval lengths. AIC selection resulted in models in which the parameters were estimated with relatively little bias. However, these models exhibited asymptotic sampling variances that were somewhat too small, and achieved confidence interval coverage that was somewhat below the nominal level. In contrast, CAIC-selected models were too simple, the parameter estimators were often substantially biased, the asymptotic sampling variances were substantially too small and the achieved coverage was often substantially below the nominal level. An example case illustrates a pattern: with 20 capture occasions, 300 previously unmarked animals are released at each occasion, and the survival and capture probabilities in the control group on each occasion were 0.9 and 0.8 respectively using model H . There was a strong acute treatment effect 3 on the first survival and first capture probability ( p ), and smaller, chronic effects 1 2 on the second and third survival probabilities ( and ) as well as on the second capture 2 3 probability ( p ); the sample size for each repetition was approximately 55 000. CAIC 3 selection led to a model with exactly these effects in only nine of the 1000 repetitions, compared with 467 times under AIC selection. Under CAIC selection, even the two acute effects were detected only 555 times, compared with 998 for AIC selection. AIC selection exhibited a balance between underfitted and overfitted models (270 versus 263), while CAIC tended strongly to select underfitted models. CAIC-selected models were overly parsimonious and poor as a basis for statistical inferences about important model parameters or structure. We recommend the use of the AIC and not the CAIC for analysis and inference from capture-recapture data sets.},
	number = {2},
	urldate = {2018-05-11},
	journal = {Journal of Applied Statistics},
	author = {Anderson, D. R. and Burnham, K. P. and White, G. C.},
	month = apr,
	year = {1998},
	pages = {263--282}
}

@book{stein_interpolation_1999,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Interpolation of {Spatial} {Data}: {Some} {Theory} for {Kriging}},
	isbn = {978-0-387-98629-6},
	shorttitle = {Interpolation of {Spatial} {Data}},
	url = {//www.springer.com/us/book/9780387986296},
	abstract = {Prediction of a random field based on observations of the random field at some set of locations arises in mining, hydrology, atmospheric sciences, and geography. Kriging, a prediction scheme defined as any prediction scheme that minimizes mean squared prediction error among some class of predictors under a particular model for the field, is commonly used in all these areas of prediction. This book summarizes past work and describes new approaches to thinking about kriging.},
	language = {en},
	urldate = {2018-05-14},
	publisher = {Springer-Verlag},
	author = {Stein, Michael L.},
	year = {1999}
}

@book{noauthor_m._nodate,
	title = {M. {Abramowitz} and {I}. {A}. {Stegun}. {Handbook} of mathematical functions},
	url = {http://people.math.sfu.ca/ cbm/aands/intro.htm},
	urldate = {2018-05-15}
}

@book{abramowitz_handbook_1974,
	address = {New York, NY, USA},
	title = {Handbook of {Mathematical} {Functions}, {With} {Formulas}, {Graphs}, and {Mathematical} {Tables},},
	isbn = {978-0-486-61272-0},
	publisher = {Dover Publications, Inc.},
	author = {Abramowitz, Milton},
	year = {1974}
}

@article{reiss_smoothing_2009,
	title = {Smoothing {Parameter} {Selection} for a {Class} of {Semiparametric} {Linear} {Models}},
	volume = {71},
	abstract = {Berkeley Electronic Press Selected Works, Spline-based approaches to nonparametric and semiparametric regression, as well as to regression of scalar outcomes on functional predictors, entail choosing a parameter controlling the extent to which roughness of the fitted function is penalized. In this paper we demonstrate that the equations determining two popular methods for smoothing parameter selection, generalized cross-validation and restricted maximum likelihood, share a similar form that allows us to prove several results common to both, and to derive a condition under which they yield identical values. These ideas are illustrated by application of functional principal component regression, a method for regressing scalars on functions, to two chemometric data sets.},
	language = {en},
	number = {2},
	urldate = {2018-06-02},
	journal = {JRSS-B},
	author = {Reiss, Philip T. and Ogden, R. Todd},
	year = {2009},
	file = {Full Text PDF:/Users/dorabeedeng/Zotero/storage/WNMWF2CS/Reiss and Ogden - 2009 - Smoothing Parameter Selection for a Class of Semip.pdf:application/pdf;Snapshot:/Users/dorabeedeng/Zotero/storage/M2GS4UIC/1.html:text/html}
}

@inproceedings{dalalyan_aggregation_2007,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Aggregation by {Exponential} {Weighting} and {Sharp} {Oracle} {Inequalities}},
	isbn = {978-3-540-72925-9 978-3-540-72927-3},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-72927-3_9},
	doi = {10.1007/978-3-540-72927-3_9},
	abstract = {In the present paper, we study the problem of aggregation under the squared loss in the model of regression with deterministic design. We obtain sharp oracle inequalities for convex aggregates defined via exponential weights, under general assumptions on the distribution of errors and on the functions to aggregate. We show how these results can be applied to derive a sparsity oracle inequality.},
	language = {en},
	urldate = {2018-06-28},
	booktitle = {Learning {Theory}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Dalalyan, Arnak S. and Tsybakov, Alexandre B.},
	month = jun,
	year = {2007},
	pages = {97--111},
	file = {Snapshot:/Users/dorabeedeng/Zotero/storage/G4QY7LY4/978-3-540-72927-3_9.html:text/html}
}

@misc{press_gaussian_2006,
	title = {Gaussian {Processes} for {Machine} {Learning}},
	url = {https://mitpress.mit.edu/books/gaussian-processes-machine-learning},
	abstract = {A comprehensive and self-contained introduction to Gaussian processes, which provide a principled, practical, probabilistic approach to learning in kernel machines.
                Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received increased attention in the machine-learning community over the past decade, and this book provides a long-needed systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised-learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and a classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support-vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises, and code and datasets are available on the Web. Appendixes provide mathematical background and a discussion of Gaussian Markov processes.},
	language = {en},
	urldate = {2018-06-28},
	journal = {The MIT Press},
	author = {Press, The MIT},
	year = {2006},
	file = {Snapshot:/Users/dorabeedeng/Zotero/storage/X9GNIZQK/gaussian-processes-machine-learning.html:text/html}
}